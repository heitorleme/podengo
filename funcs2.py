import os
import re
import io
import cv2
import math
import time
import json
import base64
import hashlib
import shutil
import random
import mimetypes
import threading
import subprocess
import tempfile
import warnings
from math import ceil
from pathlib import Path
from typing import Any, Iterable, Optional, List, Tuple, Set, Dict, Callable
from urllib.parse import urlparse, urlunparse, urlsplit, unquote
from tempfile import TemporaryDirectory, mkdtemp
import numpy as np

import pydantic; print(pydantic.__version__) # debug: mostra vers√£o do pydantic
import pandas as pd
import requests
import tiktoken
from apify_client import ApifyClient
from filetype import guess
from instaloader import Post
import instaloader
import http.cookiejar
from pymongo import MongoClient, UpdateOne
from datetime import datetime
from pymongo.errors import BulkWriteError
from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type
from loguru import logger
from datetime import datetime
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
import hashlib
import logging

# OpenAI (cliente moderno)
from openai import OpenAI, AsyncOpenAI, RateLimitError, APIConnectionError, APITimeoutError

# Whisper local √© opcional; n√£o exigimos para rodar (voc√™ usa a API do OpenAI)
try:
    import whisper # opcional
except Exception:
    whisper = None

warnings.filterwarnings("ignore")

# Max workers: Reduzido para ser mais conservador em ambientes com pouca RAM.
MAX_WORKERS_RAM_SAFE = 4
max_workers = os.cpu_count() or MAX_WORKERS_RAM_SAFE

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Config / Settings com fallback para st.secrets + os.environ
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from pydantic import Field, SecretStr, ValidationError
from pydantic_settings import BaseSettings, SettingsConfigDict

REQUIRED_KEYS = [
    "APIFY_KEY",
    "OPENAI_API_KEY",
    "MONGO_HOST",
    "MONGO_PORT",
    "MONGO_USER",
    "MONGO_PASSWORD",
    "MONGO_DB_NAME",
    "MONGO_URI"
]

BASE_DIR = Path(__file__).resolve().parent

class Settings(BaseSettings):
    model_config = SettingsConfigDict(
        env_file=str(BASE_DIR / ".env.txt"), # opcional local
        env_file_encoding="utf-8",
        extra="ignore",
    )

    RAPID_KEY: Optional[str] = None
    APIFY_KEY: SecretStr
    OPENAI_API_KEY: SecretStr
    OPENAI_CHAT_MODEL: str = "gpt-5-nano"
    OPENAI_TRANSCRIBE_MODEL: str = "whisper-1"

    MONGO_HOST: str
    MONGO_PORT: int
    MONGO_USER: str
    MONGO_PASSWORD: str
    MONGO_DB_NAME: str
    MONGO_URI: str

_settings_cache: Optional[Settings] = None

def get_settings() -> Settings:
    """Carrega segredos de st.secrets (se houver) + env e valida via Pydantic."""
    global _settings_cache
    if _settings_cache is not None:
        return _settings_cache

    env = dict(os.environ)

    # Merge com st.secrets (Streamlit Cloud / local .streamlit/secrets.toml)
    try:
        import streamlit as st
        if hasattr(st, "secrets"):
            for k, v in st.secrets.items():
                env.setdefault(k, str(v))
    except Exception:
        pass

    REQUIRED_KEYS_BASE = ["APIFY_KEY", "OPENAI_API_KEY", "MONGO_URI"]
    missing = [k for k in REQUIRED_KEYS_BASE if not env.get(k)]

    if missing:
        msg = (
            "Configura√ß√£o ausente. Defina as chaves obrigat√≥rias nas **App secrets** do Streamlit Cloud "
            "ou como vari√°veis de ambiente:\n - " + "\n - ".join(missing) +
            "\n\nNo Streamlit Cloud: Manage app ‚ñ∏ Settings ‚ñ∏ Secrets.\n"
        )
        raise RuntimeError(msg)

    # Instancia e deixa o Pydantic fazer cast/valida√ß√£o
    try:
        # Aceita tamb√©m as vari√°veis opcionais de modelo
        allowed = set(REQUIRED_KEYS + ["RAPID_KEY", "OPENAI_CHAT_MODEL", "OPENAI_TRANSCRIBE_MODEL"])
        data = {k: env[k] for k in env if k.upper() in allowed}
        _settings_cache = Settings(**data)
        # Converte SecretStr -> str
        _settings_cache.APIFY_KEY = _settings_cache.APIFY_KEY.get_secret_value() # type: ignore
        _settings_cache.OPENAI_API_KEY = _settings_cache.OPENAI_API_KEY.get_secret_value() # type: ignore
        return _settings_cache
    except ValidationError as e:
        raise RuntimeError(f"Falha ao validar configura√ß√µes: {e}") from e

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Clientes pregui√ßosos (OpenAI / Apify) criados on-demand
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
_apify_client: Optional[ApifyClient] = None
_openai_sync: Optional[OpenAI] = None
_openai_async: Optional[AsyncOpenAI] = None

def get_clients() -> Tuple[ApifyClient, OpenAI, AsyncOpenAI]:
    global _apify_client, _openai_sync, _openai_async
    s = get_settings()

    if _apify_client is None:
        _apify_client = ApifyClient(s.APIFY_KEY) # type: ignore[arg-type]

    if _openai_sync is None:
        _openai_sync = OpenAI(api_key=s.OPENAI_API_KEY) # type: ignore[arg-type]

    if _openai_async is None:
        _openai_async = AsyncOpenAI(api_key=s.OPENAI_API_KEY) # type: ignore[arg-type]

    return _apify_client, _openai_sync, _openai_async

# Caminho para m√≠dia tempor√°ria
media = str(BASE_DIR / "media")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Util / Logging
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
PRINT_LOCK = threading.Lock()
def _ts() -> str:
    return datetime.now().strftime("%H:%M:%S")

def tlog(msg: str) -> None:
    with PRINT_LOCK:
        print(f"[{_ts()}] {msg}", flush=True)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Mongo helpers
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
MONGO_FIELDS = ["url", "ownerUsername", "caption", "type", "post_timestamp", "transcricao", "framesDescricao", "embedding", "categoria_principal", "subcategoria", "comunidade_predita", "comunidades_proporcoes"]
MEDIA_FIELDS = ["mediaUrl", "mediaLocalPath", "mediaLocalPaths"]
OTHER_FIELDS = [
    "likesCount", "commentsCount", "videoPlayCount", "videoViewCount",
    "audio_id", "audio_snapshot", "hashtags", "mentions",
    "ai_model_data", "analise", "analise_tokens", "analise_erro"
]

MONGO_FETCH_FIELDS = MONGO_FIELDS + OTHER_FIELDS
OUTPUT_FIELDS = MONGO_FIELDS + MEDIA_FIELDS + OTHER_FIELDS

logger = logging.getLogger(__name__)

def _connect_to_mongo(
    HOST: Optional[str] = None,
    PORT: Optional[int] = None,
    USERNAME: Optional[str] = None,
    PASSWORD: Optional[str] = None,
    DATABASE: Optional[str] = None,
):
    """
    Conecta ao MongoDB usando MONGO_URI da configura√ß√£o.
    Os par√¢metros individuais (HOST, PORT, etc.) s√£o mantidos apenas para compatibilidade,
    mas s√£o ignorados se MONGO_URI estiver configurado.
    """
    try:
        s = get_settings()

        # üîπ Prioriza MONGO_URI (nova l√≥gica)
        if hasattr(s, "MONGO_URI") and s.MONGO_URI:
            client_mongo = MongoClient(s.MONGO_URI)
            db_name = s.MONGO_DB_NAME
            db = client_mongo[db_name]
            logger.info(f"Connected to MongoDB via URI: {db_name}")
            return db

        # üî∏ Fallback (modo legado)
        host = HOST or s.MONGO_HOST
        port = PORT or s.MONGO_PORT
        user = USERNAME or s.MONGO_USER
        pwd  = PASSWORD or s.MONGO_PASSWORD
        dbname = DATABASE or s.MONGO_DB_NAME

        client_mongo = MongoClient(host, port, username=user, password=pwd)
        db = client_mongo[dbname]
        logger.info(f"Connected to MongoDB (legacy mode): {dbname}")
        return db

    except Exception as e:
        logger.error(f"Error connecting to MongoDB: {e}")
        return None

def _mongo_collection():
    db = _connect_to_mongo()
    if db is None:
        raise RuntimeError("N√£o foi poss√≠vel conectar ao MongoDB (verifique segredos).")
    return db["video-transcript"]

def _load_docs_by_urls(urls: List[str], fields: List[str] = MONGO_FETCH_FIELDS) -> Dict[str, dict]:
    if not urls:
        return {}
    col = _mongo_collection()
    projection = {f: 1 for f in fields}
    projection["_id"] = 0
    cursor = col.find({"url": {"$in": urls}}, projection)
    out: Dict[str, dict] = {}
    for doc in cursor:
        u = doc.get("url")
        if u:
            # garante que todas as chaves existam (None quando ausentes)
            clean = {f: doc.get(f) for f in fields}
            clean["url"] = u
            out[u] = clean
    return out

@retry(wait=wait_exponential(multiplier=1, min=1, max=8), stop=stop_after_attempt(3))
def _upload_para_mongo(resultado: dict):
    try:
        col = _mongo_collection()
        if "url" not in resultado:
            logger.error("Erro: dicion√°rio n√£o cont√©m a chave 'url'.")
            return None
        if col.find_one({"url": resultado["url"]}):
            print(f"Item com url '{resultado['url']}' j√° existe na base. Upload ignorado.")
            return None
        col.insert_one(resultado)
        preview = str(resultado)[:20]
        print(f"Item {preview}... uploadado com sucesso")
    except Exception as e:
        logger.error(f"Erro no upload: {e}")
        return None

def upload_muitos_para_mongo(resultados: List[dict]) -> dict:
    col = _mongo_collection()
    if not resultados:
        return {"inserted": 0, "skipped": 0}

    try:
        col.create_index("url", unique=True, name="uniq_url")
    except Exception:
        pass

    resultados = [r for r in resultados if isinstance(r, dict) and r.get("url")]

    seen = set()
    deduped = []
    for r in resultados:
        u = r["url"]
        if u not in seen:
            deduped.append(r)
            seen.add(u)

    existentes = set(x["url"] for x in col.find({"url": {"$in": list(seen)}}, {"url": 1, "_id": 0}))
    novos = [r for r in deduped if r["url"] not in existentes]

    if not novos:
        return {"inserted": 0, "skipped": len(resultados)}

    try:
        res = col.insert_many(novos, ordered=False)
        return {"inserted": len(res.inserted_ids), "skipped": len(resultados) - len(novos)}
    except BulkWriteError as e:
        ok = e.details.get("nInserted", 0)
        return {"inserted": ok, "skipped": len(resultados) - ok}


regex_hashtags = re.compile(r"#\w+", re.UNICODE)
regex_mentions = re.compile(r"@\w+", re.UNICODE)

def _dedup_preservando_ordem(seq):
    seen = set()
    out = []
    for x in seq:
        if x not in seen:
            seen.add(x)
            out.append(x)
    return out

def _extract_tags_and_mentions(caption: str):
    caption = caption or ""
    hashtags = _dedup_preservando_ordem([h.lstrip("#") for h in regex_hashtags.findall(caption)])
    mentions = _dedup_preservando_ordem([m.lstrip("@") for m in regex_mentions.findall(caption)])
    return hashtags, mentions

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# √Åudio / Transcri√ß√£o
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class AudioModel:
    """Wrapper m√≠nimo para transcri√ß√£o usando API do OpenAI (whisper-1)."""
    def __init__(self, model: Optional[str] = None):
        s = get_settings()
        self.model = model or s.OPENAI_TRANSCRIBE_MODEL

    def transcribe(self, file_path: str) -> dict:
        try:
            _, client, _ = get_clients()
            with open(file_path, "rb") as f:
                r = client.audio.transcriptions.create(model=self.model, file=f)
            return r.model_dump() if hasattr(r, "model_dump") else {"text": getattr(r, "text", None)}
        except Exception as e:
            return {"error": str(e)}

def _tipo_midia(path: str) -> str:
    mime = mimetypes.guess_type(path)[0] or ""
    if mime.startswith("video"):
        return "video"
    if mime.startswith("image"):
        return "image"
    ext = path.lower()
    if ext.endswith((".mp4", ".mov", ".mpeg4", ".webm")):
        return "video"
    if ext.endswith((".png", ".jpeg", ".jpg", ".webp", ".gif", ".heic", ".heif")):
        return "image"
    kind = guess(path)
    if kind:
        if kind.mime.startswith("video"):
            return "video"
        if kind.mime.startswith("image"):
            return "image"
    return "desconhecido"

def _download_file(url: str, pasta_destino: str = media) -> str:
    os.makedirs(pasta_destino, exist_ok=True)
    nome = os.path.basename(urlparse(url).path) or "media"
    if not os.path.splitext(nome)[1]:
        nome += ".mp4" # fallback
    caminho = os.path.join(pasta_destino, nome)
    r = requests.get(url, allow_redirects=True, stream=True, timeout=30)
    r.raise_for_status()
    with open(caminho, "wb") as f:
        for chunk in r.iter_content(8192):
            f.write(chunk)
    return caminho

def _build_atempo_chain(speed: float) -> str:
    if speed <= 0:
        raise ValueError("speed deve ser > 0")
    factors: List[float] = []
    s = speed
    while s > 2.0:
        factors.append(2.0); s /= 2.0
    while s < 0.5:
        factors.append(0.5); s /= 0.5
    if not math.isclose(s, 1.0, rel_tol=1e-9, abs_tol=1e-9):
        s = max(0.5, min(2.0, s)); factors.append(s)
    if not factors:
        factors = [1.0]
    return ",".join(f"atempo={f}" for f in factors)

def _run_ffmpeg(cmd_args: list):
    proc = subprocess.run(
        cmd_args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=False, timeout=90
    )
    if proc.returncode != 0:
        err = proc.stderr.decode("utf-8", errors="ignore")
        raise RuntimeError(f"FFmpeg falhou:\n{err}")

@retry(wait=wait_exponential(multiplier=1, min=1, max=8), stop=stop_after_attempt(4))
def analyze_post(row):
    """
    Envia o post para a API e retorna (texto, total_tokens).
    Usa Chat Completions (mais est√°vel que Responses).
    """
    user_prompt = _build_post_prompt(row)

    resp = client.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ]
    )

    # sempre existe choices[0].message.content
    text = resp.choices[0].message.content.strip()

    total_tokens = resp.usage.total_tokens if resp.usage else None

    return text, total_tokens or 0

def get_video_frames(path: str, every_nth: Optional[int] = None) -> List[str]:
    """
    Extrai entre 5 e 15 frames de um v√≠deo usando FFmpeg, sem decodificar tudo com OpenCV.
    Retorna uma lista de strings base64 (imagens JPEG).
    √â de 3x a 10x mais r√°pido que a vers√£o anterior baseada em cv2.VideoCapture.
    """

    if not os.path.exists(path):
        raise FileNotFoundError(f"Arquivo de v√≠deo n√£o encontrado: {path}")

    # Determina a dura√ß√£o e o FPS
    try:
        proc = subprocess.run(
            [
                "ffprobe", "-v", "error",
                "-select_streams", "v:0",
                "-show_entries", "stream=duration,nb_frames,r_frame_rate",
                "-of", "json", path
            ],
            stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True
        )
        info = json.loads(proc.stdout.decode("utf-8", errors="ignore"))
        stream = (info.get("streams") or [{}])[0]
        duration = float(stream.get("duration", 0))
    except Exception:
        duration = 0

    # Decide quantos frames pegar
    if duration < 15:
        num_frames_target = 8
    elif duration < 60:
        num_frames_target = 10
    elif duration < 180:
        num_frames_target = 12
    else:
        num_frames_target = 15

    # Intervalo (segundos) entre frames
    if duration and duration > 0:
        interval = max(duration / num_frames_target, 0.5)
    else:
        interval = 1.0

    # Cria diret√≥rio tempor√°rio
    tmpdir = Path(tempfile.mkdtemp(prefix="frames_ffmpeg_"))
    pattern = str(tmpdir / "frame_%03d.jpg")

    # Extra√ß√£o direta com FFmpeg (1 frame a cada N segundos)
    cmd = [
        "ffmpeg",
        "-hide_banner", "-loglevel", "error",
        "-y",
        "-i", path,
        "-vf", "fps=1",
        "-q:v", "3",
        pattern
    ]

    # Ajuste: Roda o subprocesso
    try:
        subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=False)

        # L√™ os frames e converte para base64
        frames_b64: List[str] = []
        for jpg_path in sorted(tmpdir.glob("frame_*.jpg")):
            try:
                with open(jpg_path, "rb") as f:
                    frames_b64.append(base64.b64encode(f.read()).decode("utf-8"))
                if len(frames_b64) >= num_frames_target:
                    break
            except Exception:
                continue

        return frames_b64
        
    except Exception as e:
        raise RuntimeError(f"Erro na extra√ß√£o de frames por FFmpeg: {e}")
        
    finally:
        # Limpeza tempor√°ria: Garante que os JPGs sejam removidos imediatamente
        try:
            shutil.rmtree(tmpdir, ignore_errors=True)
        except Exception:
            pass

def _audio_temp_speed_from_video_ffmpeg(
    video_path: str,
    speed: float = 2.0,
    sample_rate: int = 16000,
    mono: bool = True,
    timeout: int = 120,
) -> Tuple[str, str]:
    """
    Extrai o √°udio do v√≠deo (ajustando velocidade) para um diret√≥rio tempor√°rio persistente.
    Retorna (tmpdir, caminho_wav).

    Corre√ß√µes:
    - Gera arquivo WAV √∫nico por job (UUID), evitando corrida entre workers.
    - Timeout aplicado ao ffmpeg (impede travamento).
    - Remove caching de WAV (inseguro com m√∫ltiplos workers simult√¢neos).
    """
    import tempfile
    from uuid import uuid4

    if not os.path.exists(video_path):
        raise FileNotFoundError(f"Arquivo n√£o encontrado: {video_path}")

    # üîπ Diret√≥rio tempor√°rio persistente
    tmpdir = Path(BASE_DIR) / "tmp"
    tmpdir.mkdir(exist_ok=True)

    # üîπ WAV √∫nico por job, evitando conflitos entre workers
    wav_name = f"audio_{uuid4().hex}_{speed}x.wav"
    out_wav = tmpdir / wav_name

    # üîπ Cadeia de filtros (ffmpeg s√≥ aceita atempo at√© 2x por filtro)
    atempo_chain = _build_atempo_chain(speed)

    cmd = [
        "ffmpeg", "-hide_banner", "-loglevel", "error", "-y",
        "-i", video_path, "-vn",
        "-filter:a", atempo_chain,
        "-ar", str(sample_rate),
    ]

    if mono:
        cmd += ["-ac", "1"]

    cmd += [str(out_wav)]

    # üîπ Executa ffmpeg com timeout
    try:
        subprocess.run(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=True,
            timeout=timeout
        )
    except subprocess.TimeoutExpired:
        raise RuntimeError(f"ffmpeg excedeu timeout de {timeout}s")
    except subprocess.CalledProcessError as e:
        raise RuntimeError(f"ffmpeg falhou: {e.stderr.decode(errors='ignore')}")

    return str(tmpdir), str(out_wav)

def transcrever_video_em_speed(
    video_path: str,
    speed: float = 1.5,
    sample_rate: int = 16000,
    ffmpeg_timeout: int = 120,
) -> Tuple[str, Optional[float]]:
    """
    Transcreve o √°udio de um v√≠deo aplicando speed-up e extra√ß√£o de WAV.
    Inclui:
    - timeout no ffmpeg
    - limpeza imediata do WAV
    - retorno seguro de erros
    """
    # Extra√ß√£o de √°udio ‚Äî AGORA COM TIMEOUT
    tmpdir, wav_path = _audio_temp_speed_from_video_ffmpeg(
        video_path,
        speed=speed,
        sample_rate=sample_rate,
        timeout=ffmpeg_timeout
    )

    dur_s = None
    texto = ""

    try:
        # Obt√©m dura√ß√£o (n√£o usa ffmpeg, ent√£o ok sem timeout)
        dur_s = _ffprobe_duration_seconds(wav_path)

        # Chamada ao modelo de transcri√ß√£o
        resp = AudioModel().transcribe(wav_path)

        if isinstance(resp, dict):
            texto = resp.get("text", "") or ""
        else:
            texto = resp or ""

        return texto, dur_s

    except Exception as e:
        return f"[Erro transcri√ß√£o] {e}", dur_s

    finally:
        # Limpeza do WAV gerado
        try:
            Path(wav_path).unlink(missing_ok=True)
            tlog(f"[LIMPEZA √ÅUDIO] üóëÔ∏è {Path(wav_path).name} removido ap√≥s transcri√ß√£o")
        except Exception as e:
            tlog(f"[WARN] falha ao remover √°udio tempor√°rio {Path(wav_path).name}: {e}")

def _ffprobe_duration_seconds(path: str) -> Optional[float]:
    try:
        proc = subprocess.run(
            ["ffprobe", "-v", "error", "-show_entries", "format=duration",
             "-of", "default=noprint_wrappers=1:nokey=1", path],
            stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True
        )
        out = proc.stdout.decode("utf-8", errors="ignore").strip()
        return float(out) if out else None
    except Exception:
        return None

# Heur√≠stica: tokens "equivalentes" por tile 512x512
TOKENS_PER_TILE = 85

def _estimate_image_tokens_from_b64_list(frames_b64: List[str], tokens_per_tile: int = TOKENS_PER_TILE) -> Tuple[int, int]:
    """
    Retorna (num_images, estimated_image_tokens) calculando tiles de 512x512 para cada imagem.
    Se falhar ao decodificar alguma imagem, assume 1 tile para ela.
    """
    total_tiles = 0
    num_images = 0
    for b64 in frames_b64:
        try:
            data = base64.b64decode(b64)
            nparr = np.frombuffer(data, np.uint8)
            img = cv2.imdecode(nparr, cv2.IMREAD_UNCHANGED)
            if img is None:
                tiles = 1
            else:
                h, w = img.shape[:2]
                tiles = ceil(w / 512) * ceil(h / 512)
            total_tiles += tiles
        except Exception:
            total_tiles += 1
        num_images += 1
    return num_images, total_tiles * tokens_per_tile

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Apify / Scrapers
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
INSTAGRAM_ACTOR = "apify/instagram-scraper"
TIKTOK_ACTOR = "clockworks/tiktok-scraper"
POLL_SEC = 2 # polling em segundos

IG_PROFILE_RESERVED = {"p", "reel", "reels", "stories", "explore", "tv"}

APIFY_KV_COOLDOWN_SEC = 2 # tempo para o Apify materializar o arquivo no KV ap√≥s o dataset
APIFY_KV_MAX_RETRY = 6 # tentativas (exponenciais)
APIFY_KV_INITIAL_DELAY = 2 # segundos

def _download_file_with_retry(url: str, pasta_destino: str = media,
                              max_retry: int = APIFY_KV_MAX_RETRY,
                              initial_delay: int = APIFY_KV_INITIAL_DELAY) -> str:
    """
    Baixa com retries exponenciais, tratando 404/403 de propaga√ß√£o do KV do Apify.
    Retorna caminho local ou lan√ßa a √∫ltima exce√ß√£o se falhar em todas as tentativas.
    """
    delay = initial_delay
    last_err = None
    for attempt in range(1, max_retry + 1):
        try:
            return _download_file(url, pasta_destino=pasta_destino)
        except requests.HTTPError as e:
            status = getattr(e.response, "status_code", None)
            last_err = e
            # 404/403 s√£o os mais comuns durante a propaga√ß√£o; aguarda e tenta de novo
            if status in (403, 404):
                tlog(f"[DL][retry {attempt}/{max_retry}] {status} para {url.split('?')[0]} ‚Äî aguardando {delay}s‚Ä¶")
                time.sleep(delay)
                delay = min(delay * 2, 30)
                continue
            # outros status ‚Üí n√£o insiste
            raise
        except Exception as e:
            last_err = e
            tlog(f"[DL][retry {attempt}/{max_retry}] erro '{type(e).__name__}' em {url.split('?')[0]} ‚Äî aguardando {delay}s‚Ä¶")
            time.sleep(delay)
            delay = min(delay * 2, 30)
    # se chegou aqui, esgotou
    raise last_err if last_err else RuntimeError("Falha desconhecida no download")


def _any_post_url(i: dict) -> str:
    for k in ("submittedVideoUrl", "webVideoUrl", "url", "shareUrl", "inputUrl"):
        v = i.get(k)
        if isinstance(v, str) and v.strip():
            return v.strip()
    return ""

def _is_ig_profile_url(u: Optional[str]) -> bool:
    if not u or "instagram.com" not in u:
        return False
    try:
        p = urlparse(u)
        segs = [s for s in (p.path or "").split("/") if s]
        # perfil simples: /<username>[/]
        return len(segs) == 1 and segs[0].lower() not in IG_PROFILE_RESERVED
    except Exception:
        return False

def _normalize_ig_profile_url(u: str) -> str:
    # for√ßa https e barra final
    if not re.match(r"^https?://", u, flags=re.I):
        u = "https://" + u
    p = urlparse(u.strip())
    path = (p.path or "/").rstrip("/") + "/"
    return urlunparse(("https", p.netloc.lower(), path, "", "", ""))

def _ig_post_url_from_item(i: dict) -> str:
    # tenta campos comuns; se s√≥ tiver shortCode, monta URL can√¥nica
    if isinstance(i.get("inputUrl"), str) and i["inputUrl"]:
        return i["inputUrl"]
    if isinstance(i.get("url"), str) and i["url"]:
        return i["url"]
    sc = i.get("shortCode")
    if isinstance(sc, str) and sc:
        return f"https://www.instagram.com/p/{sc}/"
    return ""

def _normalize_url(u: Optional[str]) -> Optional[str]:
    if not u:
        return None
    p = urlparse(u)
    # remove query/fragment e barra final
    path = p.path.rstrip("/")
    return urlunparse((p.scheme.lower(), p.netloc.lower(), path, "", "", ""))

def _post_identity(u: Optional[str]) -> Optional[Tuple[str, str]]:
    """
    Retorna uma identidade can√¥nica para merge:
      ("ig", <shortcode>) | ("tt", <video_id>) | ("url", <normalized_url>)
    """
    if not u:
        return None
    if "instagram.com" in u:
        sc = _shortcode(u)
        if sc:
            return ("ig", sc)
    if "tiktok.com" in u:
        vid = _tt_id(u)
        if vid:
            return ("tt", vid)
    nu = _normalize_url(u)
    return ("url", nu) if nu else None

def _shortcode(u: str) -> Optional[str]:
    m = re.search(r"instagram\.com/(?:reel|p)/([^/?#]+)", u, re.I)
    return m.group(1) if m else None

def _page_items(page) -> List[dict]:
    if hasattr(page, "items"):
        return getattr(page, "items") or []
    if hasattr(page, "model_dump"):
        try:
            data = page.model_dump()
            if isinstance(data, dict):
                return data.get("items", []) or []
        except Exception:
            pass
    if isinstance(page, dict):
        return page.get("items", []) or []
    return []

def _safe_filename_from_url(u: str) -> str:
    parsed = urlparse(u)
    base = os.path.basename(parsed.path) or ""
    if not base or "." not in base:
        h = hashlib.sha256(u.encode("utf-8")).hexdigest()[:16]
        base = f"static_{h}.mp4"
    return base

def _download_static_resource(u: str, pasta_destino: str) -> str:
    os.makedirs(pasta_destino, exist_ok=True)
    fname = _safe_filename_from_url(u)
    dest_path = os.path.join(pasta_destino, fname)
    if os.path.exists(dest_path) and os.path.getsize(dest_path) > 0:
        return dest_path
    with requests.get(u, stream=True, timeout=60) as r:
        r.raise_for_status()
        with open(dest_path, "wb") as f:
            for chunk in r.iter_content(chunk_size=1024 * 256):
                if chunk:
                    f.write(chunk)
    return dest_path

def _fetch_static_resources(urls_static: List[str], max_results: int) -> List[Dict[str, Optional[str]]]:
    posts = list(dict.fromkeys(urls_static))
    if not posts:
        tlog("[STATIC] Nenhuma URL est√°tica v√°lida encontrada.")
        return []
    out: List[Dict[str, Optional[str]]] = []
    for u in posts:
        if len(out) >= max_results:
            tlog(f"[STATIC] ‚õî Atingiu max_results={max_results}")
            break
        try:
            local_path = None
            try:
                local_path = _download_static_resource(u, pasta_destino=media)
            except Exception as e:
                tlog(f"[STATIC] ‚ö†Ô∏è Falha ao baixar {u}: {e}")
            payload: Dict[str, Optional[str]] = {"url": u, "mediaUrl": u}
            if local_path:
                payload["mediaLocalPath"] = local_path
                payload["mediaLocalPaths"] = [local_path]
            else:
                payload["mediaLocalPath"] = None
                payload["mediaLocalPaths"] = None
            out.append(payload)
        except Exception as e:
            tlog(f"[STATIC] ‚ùå Erro processando {u}: {e}")
    tlog(f"[STATIC] üèÅ Conclu√≠do com {len(out)} resultado(s)")
    return out

def _fetch_instagram(urls_instagram: List[str], api_token: str, max_results: int) -> List[Dict[str, Optional[str]]]:
    apify_client, _, _ = get_clients()
    posts = [u for u in urls_instagram if _shortcode(u)]
    posts = list(dict.fromkeys(posts))
    if not posts:
        tlog("[IG] Nenhuma URL de post v√°lida encontrada.")
        return []
    tlog(f"[IG] ‚ñ∂Ô∏è Iniciando run √∫nico com {len(posts)} URL(s) | actor={INSTAGRAM_ACTOR}")
    run = apify_client.actor(INSTAGRAM_ACTOR).start(
        run_input={
            "directUrls": posts,
            "resultsType": "details",
            "addParentData": False,
        }
    )
    run_id = run.get("id")
    if not run_id:
        tlog("[IG] ‚ùå N√£o recebi run_id; abortando.")
        return []
    wanted_sc = {s for s in (_shortcode(u) for u in posts) if s}
    wanted_set = set(posts)
    ds = None
    ds_id: Optional[str] = None
    offset = 0
    out: List[Dict[str, Optional[str]]] = []
    out_audio: List[Dict[str, Optional[str]]] = []
    no_new_ticks = 0

    while True:
        info = apify_client.run(run_id).get()
        status = info.get("status")
        if not ds_id:
            ds_id = info.get("defaultDatasetId")
            if ds_id:
                ds = apify_client.dataset(ds_id)
                tlog(f"[IG] üü° status={status}, dataset={ds_id}")
        if ds_id:
            page = ds.list_items(limit=200, offset=offset, clean=True)
            items = _page_items(page)
            if items:
                tlog(f"[IG] üì• +{len(items)} novos item(ns) (offset {offset}‚Üí{offset+len(items)})")
                offset += len(items)

                def _match(i: dict) -> bool:
                    iu = i.get("inputUrl") or i.get("url") or ""
                    if iu in wanted_set:
                        return True
                    sc_i = i.get("shortCode") or _shortcode(iu)
                    return bool(sc_i and sc_i in wanted_sc)

                items = [i for i in items if _match(i)]

                def _ensure_list(x):
                    if not x: return []
                    if isinstance(x, (list, tuple)): return list(x)
                    return [x]

                for it in items:
                    try:
                        m_urls = []
                        if it.get("videoUrl"):
                            m_urls.append(it["videoUrl"])
                        if it.get("displayUrl"):
                            m_urls.append(it["displayUrl"])
                        if it.get("images"):
                            m_urls.extend(it["images"] if isinstance(it["images"], list) else [it["images"]])

                        m_urls = _ensure_list(m_urls)
                        local_paths: List[str] = []
                        for mu in m_urls:
                            try:
                                lp = _download_file(mu, pasta_destino=media)
                                if lp:
                                    local_paths.append(lp)
                            except Exception as e:
                                tlog(f"[IG] ‚ö†Ô∏è Falha ao baixar {mu}: {e}")
                        # extrai info de √°udio de forma segura
                        _audio = it.get("musicInfo", {}) or {}
                        _raw_audio_id = _audio.get("audio_id")
                        audio_id = f"ig_{_raw_audio_id}" if _raw_audio_id is not None else None
                        artist_name = _audio.get("artist_name")
                        song_name = _audio.get("song_name")

                        audio_snapshot = None

                        if audio_id is not None or artist_name is not None or song_name is not None:
                            audio_snapshot = {
                                "author_name": artist_name,
                                "audio_name": song_name,
                                "plataforma": "Instagram",
                            }

                        caption = it.get("caption")
                        timestamp = it.get("timestamp")
                        hashtags, mentions = _extract_tags_and_mentions(caption)
                        
                        out.append({
                            "url": _ig_post_url_from_item(it) or _any_post_url(it),
                            "ownerUsername": it.get("ownerUsername"),
                            "likesCount": it.get("likesCount"),
                            "commentsCount": it.get("commentsCount"),
                            "caption": caption,
                            "type": it.get("type"),
                            "post_timestamp": timestamp,
                            "videoPlayCount": it.get("videoPlayCount"),
                            "videoViewCount": it.get("videoViewCount"),
                            "hashtags": hashtags,
                            "mentions": mentions,
                            "audio_id": audio_id,
                            "audio_snapshot": audio_snapshot,
                            "mediaUrl": m_urls if len(m_urls) > 1 else (m_urls[0] if m_urls else None),
                            "mediaLocalPaths": local_paths if len(local_paths) > 1 else None,
                            "mediaLocalPath": local_paths[0] if local_paths else None,
                        })

                        if len(out) % 25 == 0:
                            tlog(f"[IG] ‚úÖ {len(out)} item(ns) processado(s)")
                        if len(out) >= max_results:
                            tlog(f"[IG] ‚õî Atingiu max_results={max_results}")
                            return out[:max_results]
                    except Exception as e:
                        tlog(f"[IG] ‚ùå Erro processando item: {e}")
                no_new_ticks = 0
            else:
                no_new_ticks += 1

        if status in {"RUNNING", "READY"}:
            tlog(f"[IG] ‚è≥ status={status}, recebidos={len(out)}")
        else:
            tlog(f"[IG] üß≠ status final={status}, recebidos={len(out)}")
            if no_new_ticks >= 2:
                break
        time.sleep(POLL_SEC)

    tlog(f"[IG] üèÅ Conclu√≠do com {len(out)} resultado(s)")
    return out

def _fetch_instagram_from_profiles(
    profile_urls: List[str],
    api_token: str,
    results_limit: int = 5,
    max_results: int = 1000,
) -> List[Dict[str, Optional[str]]]:
    """
    Recebe URLs de perfil (instagram.com/<user>/), roda 1 run do Apify com resultsType=posts
    e transforma os itens retornados no mesmo formato do _fetch_instagram.
    """

    def _ig_permalink_from_item(it: dict) -> Optional[str]:
        sc = it.get("shortCode") or _shortcode(it.get("url") or it.get("inputUrl") or "")
        if sc:
            return f"https://www.instagram.com/p/{sc}/"
        return it.get("url") or it.get("inputUrl")

    apify_client, _, _ = get_clients()
    profiles = list(dict.fromkeys(_normalize_ig_profile_url(u) for u in profile_urls if _is_ig_profile_url(u)))
    if not profiles:
        tlog("[IG][PROFILES] Nenhum perfil v√°lido.")
        return []

    tlog(f"[IG][PROFILES] ‚ñ∂Ô∏è Run √∫nico ({len(profiles)} perfis) | limit={results_limit}")
    run = apify_client.actor(INSTAGRAM_ACTOR).start(
        run_input={
            "addParentData": False,
            "directUrls": profiles,
            "enhanceUserSearchWithFacebookPage": False,
            "isUserReelFeedURL": False,
            "isUserTaggedFeedURL": False,
            "resultsLimit": int(results_limit),
            "resultsType": "posts",
        }
    )
    run_id = run.get("id")
    if not run_id:
        tlog("[IG][PROFILES] ‚ùå Sem run_id.")
        return []

    ds = None
    ds_id = None
    offset = 0
    out: List[Dict[str, Optional[str]]] = []
    no_new_ticks = 0

    while True:
        info = apify_client.run(run_id).get()
        status = info.get("status")
        if not ds_id:
            ds_id = info.get("defaultDatasetId")
            if ds_id:
                ds = apify_client.dataset(ds_id)
                tlog(f"[IG][PROFILES] üü° status={status}, dataset={ds_id}")

        if ds_id:
            page = ds.list_items(limit=200, offset=offset, clean=True)
            items = _page_items(page)
            if items:
                tlog(f"[IG][PROFILES] üì• +{len(items)} (offset {offset}‚Üí{offset+len(items)})")
                offset += len(items)
                for it in items:
                    try:
                        # m√≠dia (coleta m√∫ltiplas poss√≠veis fontes)
                        def _ensure_list(x):
                            if not x: return []
                            if isinstance(x, (list, tuple)): return list(x)
                            return [x]

                        m_urls: List[str] = []
                        if it.get("videoUrl"):
                            m_urls.append(it["videoUrl"])
                        if it.get("displayUrl"):
                            m_urls.append(it["displayUrl"])
                        if it.get("images"):
                            m_urls.extend(_ensure_list(it["images"]))

                        # deduplica√ß√£o preservando ordem
                        seen_mu = set()
                        m_urls = [u for u in m_urls if not (u in seen_mu or seen_mu.add(u))]

                        local_paths: List[str] = []
                        for mu in m_urls:
                            try:
                                lp = _download_file(mu, pasta_destino=media)
                                if lp:
                                    local_paths.append(lp)
                            except Exception as e:
                                tlog(f"[IG][PROFILES] ‚ö†Ô∏è Falha download {mu}: {e}")

                        # √°udio (se dispon√≠vel)
                        _audio = it.get("musicInfo", {}) or {}
                        _raw_audio_id = _audio.get("audio_id")
                        audio_id = f"ig_{_raw_audio_id}" if _raw_audio_id is not None else None
                        artist_name = _audio.get("artist_name")
                        song_name = _audio.get("song_name")
                        audio_snapshot = None
                        if audio_id is not None or artist_name is not None or song_name is not None:
                            audio_snapshot = {
                                "author_name": artist_name,
                                "audio_name": song_name,
                                "plataforma": "Instagram",
                            }

                        caption = it.get("caption") or ""
                        hashtags, mentions = _extract_tags_and_mentions(caption)

                        permalink = _ig_permalink_from_item(it)

                        owner = it.get("ownerUsername") or it.get("username")
                        source_profile = (
                            _normalize_ig_profile_url(f"https://www.instagram.com/{owner}/") if owner else None
                        )
                        timestamp = it.get("timestamp")
                        row = {
                            "url": permalink,
                            "ownerUsername": owner,
                            "likesCount": it.get("likesCount"),
                            "commentsCount": it.get("commentsCount"),
                            "caption": caption,
                            "hashtags": hashtags,
                            "mentions": mentions,
                            "type": it.get("type") or ("Video" if it.get("isVideo") else "Image"),
                            "post_timestamp": timestamp,
                            "videoPlayCount": it.get("videoPlayCount") or it.get("videoViewCount"),
                            "videoViewCount": it.get("videoViewCount") or it.get("videoPlayCount"),
                            "audio_id": audio_id,
                            "audio_snapshot": audio_snapshot,
                            "mediaUrl": m_urls if len(m_urls) > 1 else (m_urls[0] if m_urls else None),
                            "mediaLocalPaths": local_paths if len(local_paths) > 1 else None,
                            "mediaLocalPath": local_paths[0] if local_paths else None,
                            "_source_profile": source_profile,
                        }
                        out.append(row)

                        if len(out) >= max_results:
                            tlog(f"[IG][PROFILES] ‚õî max_results atingido.")
                            return out[:max_results]
                    except Exception as e:
                        tlog(f"[IG][PROFILES] ‚ùå Erro processando item: {e}")
                no_new_ticks = 0
            else:
                no_new_ticks += 1

        if status in {"RUNNING", "READY"}:
            tlog(f"[IG][PROFILES] ‚è≥ status={status}, recebidos={len(out)}")
        else:
            tlog(f"[IG][PROFILES] üß≠ status final={status}, total={len(out)}")
            if no_new_ticks >= 2:
                break
        time.sleep(POLL_SEC)

    tlog(f"[IG][PROFILES] üèÅ Conclu√≠do com {len(out)} resultado(s)")
    return out

# 4) (opcional) De-dup no final de fetch_social_post_summary_async, antes do return:
def _dedup_results(rows: List[dict]) -> List[dict]:
    seen = set()
    out = []
    for r in rows:
        key = (r.get("url"), r.get("ownerUsername"), r.get("timestamp"))
        if key in seen:
            continue
        seen.add(key)
        out.append(r)
    return out

TT_PROFILE_RESERVED = {"tag", "music", "hashtag", "trending", "discover", "live"}

def _is_tt_profile_url(u: Optional[str]) -> bool:
    if not u or "tiktok.com" not in u:
        return False
    try:
        p = urlparse(u)
        segs = [s for s in (p.path or "").split("/") if s]
        # /@username[/]
        return len(segs) == 1 and segs[0].startswith("@") and segs[0].strip("@")
    except Exception:
        return False

def _normalize_tt_profile_url(u: str) -> str:
    if not re.match(r"^https?://", u, flags=re.I):
        u = "https://" + u
    p = urlparse(u.strip())
    path = (p.path or "/").rstrip("/") + "/"
    return urlunparse(("https", p.netloc.lower(), path, "", "", ""))

def _tt_username_from_url(u: str) -> Optional[str]:
    m = re.search(r"tiktok\.com/@([^/?#]+)", u, re.I)
    return m.group(1) if m else None

def _tt_id(u: str) -> Optional[str]:
    m = re.search(r"/video/(\d+)", u)
    return m.group(1) if m else None

def _fetch_tiktok(urls_tiktok: List[str], api_token: str, max_results: int) -> List[Dict[str, Optional[str]]]:
    apify_client, _, _ = get_clients()
    posts = [u for u in urls_tiktok if _tt_id(u)]
    posts = list(dict.fromkeys(posts))
    if not posts:
        tlog("[TT] Nenhuma URL de post v√°lida encontrada.")
        return []
    tlog(f"[TT] ‚ñ∂Ô∏è Iniciando run √∫nico com {len(posts)} URL(s) | actor={TIKTOK_ACTOR}")
    run = apify_client.actor(TIKTOK_ACTOR).start(
        run_input={
            "postURLs": posts,
            "shouldDownloadVideos": True,
            "shouldDownloadSlideshowImages": True,
        }
    )
    run_id = run.get("id")
    if not run_id:
        tlog("[TT] ‚ùå N√£o recebi run_id; abortando.")
        return []

    wanted_ids = {vid for vid in (_tt_id(u) for u in posts) if vid}
    wanted_set = set(posts)

    def _any_post_url(i: dict) -> str:
        for k in ("submittedVideoUrl", "webVideoUrl", "url", "shareUrl"):
            v = i.get(k)
            if isinstance(v, str) and v.strip():
                return v.strip()
        return ""

    def _match(i: dict) -> bool:
        iu = _any_post_url(i)
        if not iu:
            return False
        if iu in wanted_set:
            return True
        vid = _tt_id(iu)
        return bool(vid and vid in wanted_ids)

    ds = None
    ds_id: Optional[str] = None
    offset = 0
    items_total: List[dict] = []
    no_new_ticks = 0

    while True:
        info = apify_client.run(run_id).get()
        status = info.get("status")
        if not ds_id:
            ds_id = info.get("defaultDatasetId")
            if ds_id:
                ds = apify_client.dataset(ds_id)
                tlog(f"[TT] üü° status={status}, dataset={ds_id}")

        if ds_id:
            page = ds.list_items(limit=200, offset=offset, clean=True)
            page_items = _page_items(page)
            if page_items:
                tlog(f"[TT] üì• +{len(page_items)} novos item(ns) (offset {offset}‚Üí{offset+len(page_items)})")
                offset += len(page_items)
                items_total.extend(page_items)
                no_new_ticks = 0
            else:
                no_new_ticks += 1

        if status not in {"RUNNING", "READY"}:
            tlog(f"[TT] üß≠ status final={status}, recebidos={len(items_total)}")
            if no_new_ticks >= 2:
                break
        else:
            tlog(f"[TT] ‚è≥ status={status}, recebidos parciais={len(items_total)}")
        time.sleep(POLL_SEC)

    if not items_total:
        tlog("[TT] ‚ö†Ô∏è Nenhum item retornado pelo actor.")
        return []

    tlog("[TT] ‚è± aguardando 2s antes de iniciar downloads‚Ä¶")
    time.sleep(2)

    out: List[Dict[str, Optional[str]]] = []
    for it in (i for i in items_total if _match(i)):
        try:
            def _ensure_list(x):
                if not x: return []
                if isinstance(x, (list, tuple)): return list(x)
                return [x]

            media_candidates: List[str] = []
            media_candidates += [u for u in _ensure_list(it.get("mediaUrls")) if isinstance(u, str) and u.strip()]
            dl_root = it.get("downloadAddr")
            if isinstance(dl_root, str) and dl_root.strip():
                media_candidates.append(dl_root.strip())
            dl_vm = (it.get("videoMeta") or {}).get("downloadAddr")
            if isinstance(dl_vm, str) and dl_vm.strip():
                media_candidates.append(dl_vm.strip())

            seen = set()
            media_candidates = [u for u in media_candidates if not (u in seen or seen.add(u))]

            local_paths: List[str] = []
            first_ok_path: Optional[str] = None
            is_slideshow = bool(it.get("isSlideshow"))

            for mu in media_candidates:
                try:
                    lp = _download_file(mu, pasta_destino=media)
                    if lp and os.path.isfile(lp):
                        local_paths.append(lp)
                        if not first_ok_path:
                            first_ok_path = lp
                        tlog(f"[TT][DL] ‚úÖ ok ‚Üí {os.path.basename(lp)}")
                        if not is_slideshow:
                            break
                    else:
                        tlog(f"[TT][DL] ‚ö†Ô∏è sem arquivo ap√≥s download: {mu.split('?')[0]}")
                except Exception as e:
                    tlog(f"[TT][DL] ‚ö†Ô∏è falhou: {mu.split('?')[0]} | {e}")

            _type = "Slideshow" if is_slideshow or len(media_candidates) > 1 else "Video"

            # --- musicMeta seguro + vari√°veis corretas
            mm = it.get("musicMeta") or {}
            _raw_music_id = mm.get("musicId")
            audio_id: Optional[str] = None
            author_name: Optional[str] = None
            audio_name: Optional[str] = None
            plataforma = "Tiktok"

            if _raw_music_id is not None:
                audio_id = f"tt_{_raw_music_id}"
            author_name = mm.get("musicAuthor")
            audio_name = mm.get("musicName")

            audio_snapshot = None
            if audio_id is not None or author_name is not None or audio_name is not None:
                audio_snapshot = {
                    "author_name": author_name,
                    "audio_name": audio_name,
                    "plataforma": plataforma
                }

            caption = (it.get("text") or it.get("description") or "")
            timestamp = (it.get("createTimeISO") or it.get("createTime") or "")
            hashtags, mentions = _extract_tags_and_mentions(caption)
            out.append({
                "url": it.get("inputUrl") or _any_post_url(it),
                "ownerUsername": ((it.get("authorMeta") or {}).get("name")) or it.get("authorUniqueId"),
                "likesCount": it.get("diggCount"),
                "commentsCount": it.get("commentCount"),
                "caption": caption,
                "type": _type,
                "post_timestamp": timestamp,
                "videoPlayCount": it.get("playCount"),
                "videoViewCount": it.get("playCount"),
                "hashtags": hashtags,
                "mentions": mentions,
                "audio_id": audio_id,
                "audio_snapshot": audio_snapshot,
                "mediaUrl": media_candidates if len(media_candidates) > 1 else (media_candidates[0] if media_candidates else None),
                "mediaLocalPaths": local_paths if len(local_paths) > 1 else None,
                "mediaLocalPath": first_ok_path,
            })

            if media_candidates and not first_ok_path:
                tlog(f"[TT] ‚ö†Ô∏è Nenhum arquivo salvo ‚Äî candidatos={len(media_candidates)}; ex={media_candidates[0].split('?')[0]}")

            if len(out) >= max_results:
                tlog(f"[TT] ‚õî Atingiu max_results={max_results}")
                return out[:max_results]
        except Exception as e:
            keys = sorted(list(it.keys()))
            tlog(f"[TT] ‚ùå Erro processando item: {e} | keys={keys}")

    tlog(f"[TT] üèÅ Conclu√≠do com {len(out)} resultado(s)")
    return out

def _fetch_tiktok_from_profiles(
    profile_urls: List[str],
    api_token: str,
    results_limit: int = 5,
    max_results: int = 1000,
    post_urls_extra: Optional[List[str]] = None,
) -> List[Dict[str, Optional[str]]]:
    apify_client, _, _ = get_clients()

    # 1) Aceita S√ì links completos de perfil e extrai usernames, mapeando para a URL normalizada
    username_to_profile_url: Dict[str, str] = {}
    ignorados: List[str] = []
    for raw in profile_urls or []:
        if not isinstance(raw, str) or not raw.strip():
            ignorados.append(str(raw)); continue
        u = raw.strip()
        if "tiktok.com" not in u:
            ignorados.append(u); continue
        if not re.match(r"^https?://", u, flags=re.I):
            u = "https://" + u
        if not _is_tt_profile_url(u):
            ignorados.append(u); continue
        u_norm = _normalize_tt_profile_url(u) # sempre com barra final
        un = _tt_username_from_url(u_norm)
        if un:
            username_to_profile_url.setdefault(un, u_norm)
        else:
            ignorados.append(u)

    usernames = list(username_to_profile_url.keys())
    if ignorados:
        tlog(f"[TT][PROFILES] Ignorados (n√£o s√£o links completos): {len(ignorados)}")
    if not usernames:
        tlog("[TT][PROFILES] ‚ùå Nenhum perfil v√°lido.")
        return []

    run_input = {
        "excludePinnedPosts": False,
        "postURLs": list(dict.fromkeys((post_urls_extra or []))),
        "profiles": usernames, # apenas usernames
        "proxyCountryCode": "None",
        "resultsPerPage": int(results_limit),
        "scrapeRelatedVideos": False,
        "shouldDownloadAvatars": False,
        "shouldDownloadCovers": False,
        "shouldDownloadMusicCovers": False,
        "shouldDownloadSlideshowImages": False,
        "shouldDownloadSubtitles": False,
        "shouldDownloadVideos": True,
    }

    tlog(f"[TT][PROFILES] ‚ñ∂Ô∏è Run √∫nico ({len(usernames)} perfis) | resultsPerPage={results_limit}")
    run = apify_client.actor(TIKTOK_ACTOR).start(run_input=run_input)
    run_id = run.get("id")
    if not run_id:
        tlog("[TT][PROFILES] ‚ùå Sem run_id.")
        return []

    ds, ds_id, offset = None, None, 0
    out: List[Dict[str, Optional[str]]] = []
    cache_media_candidates: List[Tuple[int, List[str], bool]] = [] # (idx_out, candidates, is_slideshow)
    per_user_count: Dict[str, int] = {}
    no_new_ticks = 0

    while True:
        info = apify_client.run(run_id).get()
        status = info.get("status")
        if not ds_id:
            ds_id = info.get("defaultDatasetId")
            if ds_id:
                ds = apify_client.dataset(ds_id)
                tlog(f"[TT][PROFILES] üü° status={status}, dataset={ds_id}")

        if ds_id:
            page = ds.list_items(limit=200, offset=offset, clean=True)
            items = _page_items(page)
            if items:
                tlog(f"[TT][PROFILES] üì• +{len(items)} (offset {offset}‚Üí{offset+len(items)})")
                offset += len(items)

                for it in items:
                    try:
                        # associa ao perfil de origem via 'input' (username enviado)
                        src_username = (it.get("input") or "").strip() or ((it.get("authorMeta") or {}).get("name") or "").strip()
                        src_profile_url = username_to_profile_url.get(src_username) or (_normalize_tt_profile_url(f"https://www.tiktok.com/@{src_username}/") if src_username else None)

                        # candidatos de m√≠dia (prioriza downloadAddr)
                        def _ensure_list(x):
                            if not x: return []
                            if isinstance(x, (list, tuple)): return list(x)
                            return [x]
                        media_candidates: List[str] = []
                        dl_vm = (it.get("videoMeta") or {}).get("downloadAddr")
                        if isinstance(dl_vm, str) and dl_vm.strip():
                            media_candidates.append(dl_vm.strip())
                        media_candidates += [u for u in _ensure_list(it.get("mediaUrls")) if isinstance(u, str) and u.strip()]
                        seen_mu = set()
                        media_candidates = [u for u in media_candidates if not (u in seen_mu or seen_mu.add(u))]
                        is_slideshow = bool(it.get("isSlideshow"))

                        # √°udio
                        mm = (it.get("musicMeta") or {})
                        audio_id = None
                        author_name = None
                        audio_name = None
                        if isinstance(mm, dict):
                            _raw_music_id = mm.get("musicId")
                            if _raw_music_id is not None:
                                audio_id = f"tt_{_raw_music_id}"
                            author_name = mm.get("musicAuthor")
                            audio_name = mm.get("musicName")
                        audio_snapshot = None
                        if audio_id is not None or author_name is not None or audio_name is not None:
                            audio_snapshot = {"author_name": author_name, "audio_name": audio_name, "plataforma": "Tiktok"}

                        # texto/hashtags
                        caption = (it.get("text") or it.get("description") or "") or ""
                        hashtags_cap, mentions_cap = _extract_tags_and_mentions(caption)
                        h_objs = it.get("hashtags") or []
                        if isinstance(h_objs, list):
                            h_names = [h.get("name") for h in h_objs if isinstance(h, dict) and h.get("name")]
                        else:
                            h_names = []
                        hashtags = _dedup_preservando_ordem(list(hashtags_cap) + h_names)
                        mentions = mentions_cap

                        owner = ((it.get("authorMeta") or {}).get("name")) or it.get("authorUniqueId")
                        permalink = it.get("webVideoUrl") or it.get("url") or it.get("inputUrl")
                        _type = "Slideshow" if is_slideshow or len(media_candidates) > 1 else "Video"

                        row = {
                            "url": permalink,
                            "ownerUsername": owner,
                            "likesCount": it.get("diggCount"),
                            "commentsCount": it.get("commentCount"),
                            "caption": caption,
                            "hashtags": hashtags,
                            "mentions": mentions,
                            "type": _type,
                            "post_timestamp": it.get("createTimeISO") or it.get("createTime"),
                            "videoPlayCount": it.get("playCount"),
                            "videoViewCount": it.get("playCount"),
                            "audio_id": audio_id,
                            "audio_snapshot": audio_snapshot,
                            "mediaUrl": media_candidates if len(media_candidates) > 1 else (media_candidates[0] if media_candidates else None),
                            "mediaLocalPaths": None,
                            "mediaLocalPath": None,
                            "_source_profile": src_profile_url,
                        }
                        out.append(row)
                        cache_media_candidates.append((len(out) - 1, media_candidates, is_slideshow))

                        if src_username:
                            per_user_count[src_username] = per_user_count.get(src_username, 0) + 1

                        if len(out) >= max_results:
                            tlog(f"[TT][PROFILES] ‚õî max_results atingido.")
                            break
                    except Exception as e:
                        tlog(f"[TT][PROFILES] ‚ùå Erro processando item: {e}")
                # fim for items
                if len(out) >= max_results:
                    break
                no_new_ticks = 0
            else:
                no_new_ticks += 1

        if status in {"RUNNING", "READY"}:
            tlog(f"[TT][PROFILES] ‚è≥ status={status}, recebidos={len(out)}")
        else:
            tlog(f"[TT][PROFILES] üß≠ status final={status}, total={len(out)}")
            if per_user_count:
                tlog(f"[TT][PROFILES] Resumo por username: {per_user_count}")
            break
        time.sleep(POLL_SEC)

    # ‚úÖ COOLDOWN global ap√≥s o run (deixa o KV estabilizar) + retry por URL
    if out:
        tlog(f"[TT][PROFILES] ‚è± cooldown {APIFY_KV_COOLDOWN_SEC}s antes dos downloads‚Ä¶")
        time.sleep(APIFY_KV_COOLDOWN_SEC)
        for idx_out, candidates, is_slideshow in cache_media_candidates:
            if not candidates:
                continue
            local_paths: List[str] = []
            first_ok: Optional[str] = None
            for mu in candidates:
                try:
                    lp = _download_file_with_retry(mu, pasta_destino=media)
                    if lp and os.path.isfile(lp):
                        local_paths.append(lp)
                        if not first_ok:
                            first_ok = lp
                        tlog(f"[TT][PROFILES][DL] ‚úÖ ok ‚Üí {os.path.basename(lp)}")
                        if not is_slideshow:
                            break
                    else:
                        tlog(f"[TT][PROFILES][DL] ‚ö†Ô∏è sem arquivo salvo para {mu.split('?')[0]}")
                except Exception as e:
                    tlog(f"[TT][PROFILES][DL] ‚ö†Ô∏è {mu.split('?')[0]} | {e}")
            # atualiza a linha
            if local_paths:
                out[idx_out]["mediaLocalPaths"] = local_paths if len(local_paths) > 1 else None
                out[idx_out]["mediaLocalPath"] = first_ok
            else:
                out[idx_out]["mediaLocalPaths"] = None
                out[idx_out]["mediaLocalPath"] = None

    tlog(f"[TT][PROFILES] üèÅ Conclu√≠do com {len(out)} resultado(s)")
    return out

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Pipeline principal (busca ‚Üí transcri√ß√£o ‚Üí limpeza)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _partition_known_unknown_by_mongo(all_urls: List[str]) -> Tuple[Dict[str, dict], Set[str]]:
    known_map = _load_docs_by_urls(all_urls, MONGO_FETCH_FIELDS)
    known_urls = set(known_map.keys())
    unknown_set = set(u for u in all_urls if u not in known_urls)
    return known_map, unknown_set

def _merge_results_in_input_order(
    input_urls: List[str],
    known_map: Dict[str, dict],
    fetched_new_items: List[Dict[str, Optional[str]]]
) -> List[Dict[str, Optional[str]]]:
    # √çndices por identidade/URL e tamb√©m por perfil (quando vier de perfis)
    fetched_by_id: Dict[Tuple[str, str], dict] = {}
    fetched_by_norm_url: Dict[str, dict] = {}
    by_profile: Dict[str, List[dict]] = {}

     # üîí Garantir que n√£o existem itens None ou n√£o-dicts
    fetched_new_items = [it for it in (fetched_new_items or []) if isinstance(it, dict)]

    def _cand_urls(it: dict) -> List[str]:
        cand = []
        for k in ("url", "inputUrl", "webVideoUrl", "shareUrl"):
            v = it.get(k)
            if isinstance(v, str) and v.strip():
                cand.append(v.strip())
        seen = set()
        return [u for u in cand if not (u in seen or seen.add(u))]

    # indexa√ß√£o dos itens novos
    for it in fetched_new_items or []:
        sp = it.get("_source_profile")
        if isinstance(sp, str) and sp:
            by_profile.setdefault(sp, []).append(it)

        for u in _cand_urls(it):
            pid = _post_identity(u)
            if pid and pid not in fetched_by_id:
                fetched_by_id[pid] = it
            nu = _normalize_url(u)
            if nu and nu not in fetched_by_norm_url:
                fetched_by_norm_url[nu] = it

    results: List[Dict[str, Optional[str]]] = []

    for u in input_urls:
        # 1) reaproveita do Mongo
        if u in known_map:
            doc = {k: known_map[u].get(k) if k in known_map[u] else None for k in MONGO_FETCH_FIELDS}
            doc["url"] = u
            doc["_from_mongo"] = True
            doc.setdefault("hashtags", [])
            doc.setdefault("mentions", [])
            results.append(doc)
            continue

        # 2) fan-out para perfis IG/TT (n√£o criar placeholder quando n√£o houver posts)
        if _is_ig_profile_url(u) or _is_tt_profile_url(u):
            prof_key = _normalize_ig_profile_url(u) if "instagram.com" in u else _normalize_tt_profile_url(u)
            posts = by_profile.get(prof_key, [])
            for it in posts:
                row = {k: it.get(k) if k in it else None for k in OUTPUT_FIELDS}
                if not row.get("url"):
                    row["url"] = it.get("inputUrl") or u
                row["_from_mongo"] = False
                row.setdefault("hashtags", [])
                row.setdefault("mentions", [])
                results.append(row)
            # ‚ö†Ô∏è N√£o adiciona placeholder para URLs de perfil (servem apenas como semente)
            continue

        # 3) caso normal (post √∫nico)
        pid = _post_identity(u)
        it = fetched_by_id.get(pid) if pid else None
        if not it:
            it = fetched_by_norm_url.get(_normalize_url(u))

        if it:
            row = {k: it.get(k) if k in it else None for k in OUTPUT_FIELDS}
            if not row.get("url"):
                row["url"] = it.get("inputUrl") or u
            row["_from_mongo"] = False
            row.setdefault("hashtags", [])
            row.setdefault("mentions", [])
            results.append(row)
        else:
            # para posts (n√£o perfis), mantemos o placeholder
            placeholder = {"url": u, "_from_mongo": False}
            for k in OUTPUT_FIELDS:
                if k != "url":
                    placeholder[k] = None
            placeholder["hashtags"] = []
            placeholder["mentions"] = []
            results.append(placeholder)

    return results

def _tipo_midia_url(url: Optional[str]) -> str:
    if not url:
        return "desconhecido"
    p = urlparse(url)
    clean = urlunparse((p.scheme, p.netloc, p.path, "", "", ""))
    tipo = _tipo_midia(clean)
    if tipo == "desconhecido" and "dst-mp4" in url.lower():
        return "video"
    return tipo

def _pick_media_url(media: Any, media_local_path: Any = None, media_local_paths: Any = None) -> Optional[str]:
    """
    Escolhe a melhor fonte de m√≠dia para processamento.
    üîÅ Prioriza ARQUIVOS LOCAIS (necess√°rios para transcri√ß√£o e extra√ß√£o de frames),
    depois cai para URLs remotas como fallback.
    """
    # 1) Prioriza caminhos locais (o worker depende disso)
    if isinstance(media_local_path, str) and media_local_path and Path(media_local_path).exists():
        return media_local_path
    if isinstance(media_local_paths, (list, tuple)):
        for p in media_local_paths:
            if isinstance(p, str) and p and Path(p).exists():
                return p

    # 2) Fallback: URLs remotas (n√£o usadas pelo worker para abrir arquivo, mas √∫til p/ logging)
    if media:
        if isinstance(media, str) and media:
            return media
        if isinstance(media, list):
            for m in media:
                if isinstance(m, str) and m:
                    return m
        if isinstance(media, dict):
            for v in media.values():
                if isinstance(v, str) and v:
                    return v
                if isinstance(v, list):
                    for m in v:
                        if isinstance(m, str) and m:
                            return m
    return None


def _descrever_frames(frames_b64: List[str], max_imgs: int = 5, idioma: str = "pt-BR") -> Tuple[str, Optional[int], Optional[int], int, int]:
    """
    Retorna: (descricao_texto, input_tokens_text, output_tokens_text, num_images, estimated_image_tokens).
    - input/output_tokens_text: apenas TEXTO (usage da API).
    - num_images / estimated_image_tokens: estimativa baseada em tiles 512x512 por imagem.
    """
    if not frames_b64:
        return "", None, None, 0, 0
    try:
        imagens = frames_b64[:max_imgs]

        # calcula num_images e tokens estimados pelas imagens (por resolu√ß√£o)
        num_images, estimated_image_tokens = _estimate_image_tokens_from_b64_list(imagens)

        mensagens = [
            {
                "role": "user",
                "content": (
                    [{"type": "text", "text": f"Descreva em {idioma} o que aparece nestas imagens/frame de v√≠deo. "
                                              f"Foque em quem/que objetos aparecem, a√ß√µes e contexto. Seja conciso. "
                                              f"N√£o descreva cada frame individualmente; fa√ßa um resumo da tem√°tica. "
                                              f"Mencione marcas/produtos espec√≠ficos se aparecerem."}]
                    + [{"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64}"}} for b64 in imagens]
                ),
            }
        ]
        s = get_settings()
        _, client, _ = get_clients()
        # Ajuste o modelo se necess√°rio. 'gpt-5-nano' pode n√£o ser o modelo padr√£o que voc√™ queria para vis√£o.
        # Assumindo que voc√™ quis usar um modelo de vis√£o, como 'gpt-4-vision-preview'.
        # Mantendo 'gpt-5-nano' apenas para evitar quebras se o modelo for configur√°vel fora deste arquivo.
        resp = client.chat.completions.create(model=s.OPENAI_CHAT_MODEL, messages=mensagens)

        texto = (resp.choices[0].message.content or "").strip()

        # tokens de TEXTO (prompt/completion) ‚Äî imagens n√£o entram no usage
        usage = getattr(resp, "usage", None)
        input_tokens = getattr(usage, "prompt_tokens", None) if usage else None
        output_tokens = getattr(usage, "completion_tokens", None) if usage else None

        return texto, input_tokens, output_tokens, num_images, estimated_image_tokens

    except Exception as e:
        return f"[ERRO descri√ß√£o frames] {e}", None, None, 0, 0

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Transcri√ß√£o em paralelo (threads) + descri√ß√£o de frames
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _thread_worker(idx: int, media_url: Optional[str], sem: None):
    """
    Worker de processamento de v√≠deo/imagem:
      - Baixa a m√≠dia para um caminho √∫nico TEMPOR√ÅRIO por job (evita corrida)
      - Transcreve √°udio (com timeout dentro de transcrever_video_em_speed)
      - Extrai frames
      - Descreve frames
      - Remove o arquivo local no final (sem conflito entre threads)

    Retorna:
      (idx, transcricao:str|None, base64Frames:list[str],
       framesDescricao:str|None, ai_model_data:dict, erro:str|None)
    """
    ai_model_data = {
        "ai_model": "gpt-5-nano",
        "input_tokens": None,
        "output_tokens": None,
        "audio_seconds": None,
        "num_images": 0,
        "estimated_image_tokens": 0,
    }

    local_path = Path()  # definido aqui para o finally

    if not media_url:
        return idx, None, [], None, ai_model_data, "sem_media_url"

    try:
        # ---------------------------------------------------------------------
        # üîπ Caminho TEMPOR√ÅRIO √∫nico por job (correto; sem corrida entre threads)
        # ---------------------------------------------------------------------
        from uuid import uuid4
        import tempfile

        suffix = Path(media_url).suffix or ".mp4"
        fd, tmpname = tempfile.mkstemp(prefix=f"job_{uuid4().hex}_", suffix=suffix)
        os.close(fd)
        local_path = Path(tmpname)

        # ---------------------------------------------------------------------
        # üîπ Baixar a m√≠dia (se for URL)
        # ---------------------------------------------------------------------
        if media_url.startswith("http://") or media_url.startswith("https://"):
            try:
                resp = requests.get(media_url, timeout=30)
                resp.raise_for_status()
                local_path.write_bytes(resp.content)
            except Exception as e:
                return idx, None, [], None, ai_model_data, f"DownloadError: {e}"

        # ---------------------------------------------------------------------
        # üîπ Se n√£o for URL, copiar o arquivo local para o temp exclusivo
        # ---------------------------------------------------------------------
        else:
            original = Path(media_url)
            if not original.exists():
                return idx, None, [], None, ai_model_data, f"FileNotFoundError: {original.name}"
            local_path.write_bytes(original.read_bytes())

        # ---------------------------------------------------------------------
        # üîπ Detecta tipo
        # ---------------------------------------------------------------------
        ext = local_path.suffix.lower()
        is_video = ext in {".mp4", ".mov", ".m4v", ".webm", ".ts", ".mkv", ".avi"}
        is_image = ext in {".jpg", ".jpeg", ".png", ".gif", ".bmp", ".webp"}

        if not is_video and not is_image:
            kind = _tipo_midia(str(local_path))
            is_video = (kind == "video")
            is_image = (kind == "image")

        texto = None
        base64_frames = []
        frames_descricao = None

        # ---------------------------------------------------------------------
        # üîπ PROCESSAMENTO DE V√çDEO
        # ---------------------------------------------------------------------
        if is_video:
            tlog(f"[TR] üéôÔ∏è idx={idx}: processando v√≠deo {local_path.name}")

            texto_resp, dur_s = transcrever_video_em_speed(str(local_path))
            texto = texto_resp
            ai_model_data["audio_seconds"] = dur_s

            # Frames do v√≠deo
            base64_frames = get_video_frames(str(local_path))

            # Descri√ß√£o dos frames
            if base64_frames:
                secs = dur_s or 0
                dyn_max_imgs = max(3, min(15, int(round(secs / 6)) or 1))

                desc, in_tok, out_tok, num_imgs, est_img_tokens = _descrever_frames(
                    base64_frames, max_imgs=dyn_max_imgs
                )

                frames_descricao = desc
                ai_model_data["input_tokens"] = in_tok
                ai_model_data["output_tokens"] = out_tok
                ai_model_data["num_images"] = num_imgs
                ai_model_data["estimated_image_tokens"] = est_img_tokens

            tlog(f"[TR] ‚úÖ idx={idx}: conclu√≠do {local_path.name}")

        # ---------------------------------------------------------------------
        # üîπ PROCESSAMENTO DE IMAGEM
        # ---------------------------------------------------------------------
        elif is_image:
            tlog(f"[TR] üñºÔ∏è idx={idx}: processando imagem {local_path.name}")

            with local_path.open("rb") as img:
                base64_frames = [base64.b64encode(img.read()).decode("utf-8")]

            desc, in_tok, out_tok, num_imgs, est_img_tokens = _descrever_frames(
                base64_frames, max_imgs=1
            )

            frames_descricao = desc
            ai_model_data["input_tokens"] = in_tok
            ai_model_data["output_tokens"] = out_tok
            ai_model_data["num_images"] = num_imgs
            ai_model_data["estimated_image_tokens"] = est_img_tokens

            tlog(f"[TR] ‚úÖ idx={idx}: conclu√≠do {local_path.name}")

        # ---------------------------------------------------------------------
        # üîπ RETORNO PADR√ÉO
        # ---------------------------------------------------------------------
        return idx, texto, base64_frames, frames_descricao, ai_model_data, None

    except Exception as e:
        return idx, None, [], None, ai_model_data, f"{type(e).__name__}: {e}"

    finally:
        # ---------------------------------------------------------------------
        # ‚ôªÔ∏è LIMPEZA (segura ‚Äî arquivo √© exclusivo deste job)
        # ---------------------------------------------------------------------
        try:
            if local_path.is_file():
                local_path.unlink(missing_ok=True)
                tlog(f"[LIMPEZA] üóëÔ∏è {local_path.name} removido")
        except Exception as e:
            tlog(f"[WARN] falha ao remover {local_path.name}: {e}")

def anexar_transcricoes_threaded(
    resultados: List[Dict[str, Any]],
    max_workers: int = 1,
    gpu_singleton: bool = False,
    callback: Optional[Callable[[int, int], None]] = None,
) -> List[Dict[str, Any]]:
    """
    Para cada item com m√≠dia, processa v√≠deo/imagem em paralelo (ThreadPool).
    Recursos:
      - Jobs √∫nicos ‚úî
      - Sem sem√°foro ‚úî
      - Sem GPU singleton ‚úî
      - Caminhos exclusivos por job (no worker) ‚úî
      - Timeout no ffmpeg (no worker) ‚úî
      - Nenhum base64Frames armazenado ‚úî
    """

    if not resultados:
        return resultados

    # Garantir que todos os itens s√£o dicts v√°lidos
    for i, item in enumerate(resultados):
        if not isinstance(item, dict):
            tlog(f"[ERRO] resultados[{i}] n√£o √© dict dentro do worker. Substituindo por dict vazio.")
            resultados[i] = {}

    # Sem√°foro removido
    sem = None

    # ==========================================================
    # üîπ Criar lista de jobs √öNICOS (idx, url_midia)
    # ==========================================================
    jobs: List[Tuple[int, Optional[str]]] = []
    seen = set()   # evita duplicatas

    for idx, item in enumerate(resultados):
        veio_mongo = bool(item.get("_from_mongo"))

        # garante estrutura do modelo
        item.setdefault("ai_model_data", {
            "ai_model": "gpt-5-nano",
            "input_tokens": None,
            "output_tokens": None,
            "audio_seconds": None,
            "num_images": 0,
            "estimated_image_tokens": 0,
        })

        item.setdefault("base64Frames", [])

        # Se veio do mongo e j√° tem dados, pula
        if veio_mongo and (
            item.get("transcricao") is not None
            or item.get("framesDescricao") is not None
        ):
            continue

        # Pega URL/m√≠dia local
        url_midia = _pick_media_url(
            item.get("mediaUrl"),
            item.get("mediaLocalPath"),
            item.get("mediaLocalPaths"),
        )

        if url_midia:
            key = (idx, url_midia)
            if key not in seen:     # EVITA JOB DUPLICADO ‚úî
                seen.add(key)
                jobs.append((idx, url_midia))
        else:
            # falta de m√≠dia
            if item.get("transcricao") is None and item.get("framesDescricao") is None:
                item["transcricao"] = None
                item["transcricao_erro"] = "sem_media_url"
                item["framesDescricao"] = None

    if not jobs:
        return resultados

    # ==========================================================
    # üîπ Progresso seguro
    # ==========================================================
    total = len(jobs)
    concluidos = 0

    # ==========================================================
    # üîπ Executor de threads
    # ==========================================================
    with ThreadPoolExecutor(max_workers=max_workers) as ex:

        futs = [
            ex.submit(_thread_worker, idx, url, None)
            for idx, url in jobs
        ]

        for fut in as_completed(futs):

            (
                idx,
                transcricao,
                frames,
                frames_desc,
                ai_model_data,
                erro,
            ) = fut.result()

            # üîí SLAM DUNK PROTECTION: impede que idx inv√°lido cause NoneType
            if idx is None or idx < 0 or idx >= len(resultados) or resultados[idx] is None:
                tlog(f"[ERRO] Worker retornou idx inv√°lido ou item None: idx={idx}")
                continue

            # ----------------------------
            # preencher resultados[idx]
            # ----------------------------
            if transcricao is not None:
                resultados[idx]["transcricao"] = transcricao

            # n√£o guardamos base64Frames (memory-safe)
            if frames_desc is not None:
                resultados[idx]["framesDescricao"] = frames_desc

            resultados[idx]["ai_model_data"] = ai_model_data or {
                "ai_model": "gpt-5-nano",
                "input_tokens": None,
                "output_tokens": None,
                "audio_seconds": None,
            }

            if erro:
                resultados[idx]["transcricao_erro"] = erro
            else:
                resultados[idx].pop("transcricao_erro", None)

            # ----------------------------
            # progresso do callback
            # ----------------------------
            concluidos += 1
            if callback:
                try:
                    callback(concluidos, total)
                except Exception as e:
                    tlog(f"[CALLBACK] ‚ö†Ô∏è erro no callback: {e}")

    # ==========================================================
    # üîπ Garantia de estrutura ap√≥s processamento
    # ==========================================================
    for i in range(len(resultados)):
        item = resultados[i]
    
        # Se o item n√£o √© dict, cria um dict m√≠nimo e normaliza
        if not isinstance(item, dict):
            tlog(f"[ERRO] resultados[{i}] inv√°lido (None/n√£o-dict). Normalizando para estrutura m√≠nima.")
            resultados[i] = {
                "transcricao": None,
                "framesDescricao": None,
                "transcricao_erro": "item_invalido",
                "ai_model_data": {},
                "base64Frames": [],
            }
            continue
    
        # Campos b√°sicos
        item.setdefault("transcricao", None)
        item.setdefault("framesDescricao", None)
        item.setdefault("transcricao_erro", None)
    
        # For√ßa base64Frames como lista vazia (zero RAM)
        bf = item.get("base64Frames")
        if not isinstance(bf, list) or bf:
            tlog(f"[WARN] resultados[{i}].base64Frames estava preenchido ou inv√°lido. Resetando para [].")
            item["base64Frames"] = []
    
        # ai_model_data robusto
        amd = item.get("ai_model_data")
        if amd is None or not isinstance(amd, dict):
            tlog(f"[WARN] resultados[{i}].ai_model_data inv√°lido ({type(amd)}). Recriando dict vazio.")
            amd = {}
            item["ai_model_data"] = amd
    
        # Normaliza√ß√£o do conte√∫do interno
        amd.setdefault("ai_model", "gpt-5-nano")
        amd.setdefault("input_tokens", None)
        amd.setdefault("output_tokens", None)
        amd.setdefault("audio_seconds", None)
        amd.setdefault("num_images", 0)
        amd.setdefault("estimated_image_tokens", 0)
    return resultados

# ==========================================================
# 2.5Ô∏è‚É£ GERAR AN√ÅLISE GPT DOS V√çDEOS / POSTS
# ==========================================================
update_step("üìù Gerando an√°lise dos v√≠deos...")

from concurrent.futures import ThreadPoolExecutor, as_completed

def _analisar_item(i, item):
    try:
        texto, tokens = analyze_post({
            "ownerUsername": item.get("ownerUsername"),
            "caption": item.get("caption"),
            "transcricao": item.get("transcricao"),
            "framesDescricao": item.get("framesDescricao"),
        })
        item["analise"] = texto
        item["analise_tokens"] = tokens
        item["analise_erro"] = None
    except Exception as e:
        item["analise"] = None
        item["analise_tokens"] = 0
        item["analise_erro"] = str(e)
    return i

total_items = len(resultados)
done = 0

with ThreadPoolExecutor(max_workers=max_workers) as ex:
    futures = [ex.submit(_analisar_item, i, item) for i, item in enumerate(resultados)]
    for fut in as_completed(futures):
        done += 1
        progresso_total = (step + (done / total_items)) / total_steps
        _safe_progress(progresso_total, f"üìù Analisando ({done}/{total_items})...")

def anexar_analises_threaded(
    resultados: List[Dict[str, Any]],
    max_workers: int = 4,
    callback: Optional[Callable[[int, int], None]] = None,
) -> List[Dict[str, Any]]:
    """
    Para cada item, gera a an√°lise estruturada (usando analyze_post) em paralelo.
    Campos preenchidos em cada item:
      - analise: texto da an√°lise em markdown
      - analise_tokens: total de tokens usados na chamada
      - analise_erro: mensagem de erro (se houver)
    """

    if not resultados:
        return resultados

    # Garantir que todos os itens s√£o dicts v√°lidos
    for i, item in enumerate(resultados):
        if not isinstance(item, dict):
            tlog(f"[ANALISE] [ERRO] resultados[{i}] n√£o √© dict. Substituindo por dict vazio.")
            resultados[i] = {}
    
    # ==========================================================
    # üîπ Criar lista de jobs (idx) que realmente precisam de an√°lise
    # ==========================================================
    jobs: List[int] = []

    for idx, item in enumerate(resultados):
        if not isinstance(item, dict):
            continue

        veio_mongo = bool(item.get("_from_mongo"))

        # Se veio do mongo e j√° tem an√°lise, pula
        if veio_mongo and item.get("analise"):
            continue

        caption = item.get("caption") or ""
        transcricao = item.get("transcricao") or ""
        frames_desc = item.get("framesDescricao") or ""

        # Se n√£o tem absolutamente nada de texto pra analisar, marca erro e pula
        if not (caption or transcricao or frames_desc):
            if item.get("analise") is None and item.get("analise_erro") is None:
                item["analise"] = None
                item["analise_tokens"] = 0
                item["analise_erro"] = "sem_texto_para_analisar"
            continue

        jobs.append(idx)

    if not jobs:
        tlog("[ANALISE] Nenhum item eleg√≠vel para an√°lise.")
        return resultados

    total = len(jobs)
    concluidos = 0

    tlog(f"[ANALISE] Iniciando gera√ß√£o de an√°lises para {total} item(ns)...")

    # ==========================================================
    # üîπ Worker de an√°lise (executado em threads)
    # ==========================================================
    def _thread_analise(idx: int) -> Tuple[int, Optional[str], int, Optional[str]]:
        item = resultados[idx]

        # monta o "row" no formato esperado por analyze_post
        row = {
            "ownerUsername": item.get("ownerUsername") or item.get("creatorname") or "",
            "caption": item.get("caption") or "",
            "transcricao": item.get("transcricao") or "",
            "framesDescricao": item.get("framesDescricao") or "",
        }

        try:
            texto, tokens = analyze_post(row)
            erro = None
        except Exception as e:
            texto = None
            tokens = 0
            erro = str(e)
        return idx, texto, tokens, erro

    # ==========================================================
    # üîπ Executor de threads
    # ==========================================================
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futs = [ex.submit(_thread_analise, idx) for idx in jobs]

        for fut in as_completed(futs):
            idx, texto, tokens, erro = fut.result()

            if idx is None or idx < 0 or idx >= len(resultados) or resultados[idx] is None:
                tlog(f"[ANALISE] [ERRO] Worker retornou idx inv√°lido ou item None: idx={idx}")
                continue

            item = resultados[idx]

            item["analise"] = texto
            item["analise_tokens"] = tokens
            item["analise_erro"] = erro

            concluidos += 1
            if callback:
                try:
                    callback(concluidos, total)
                except Exception as e:
                    tlog(f"[ANALISE] [CALLBACK] ‚ö†Ô∏è erro no callback: {e}")

    # ==========================================================
    # üîπ Normaliza√ß√£o final da estrutura
    # ==========================================================
    for i, item in enumerate(resultados):
        if not isinstance(item, dict):
            tlog(f"[ANALISE] [ERRO] resultados[{i}] inv√°lido ap√≥s an√°lise. Normalizando.")
            resultados[i] = {
                "analise": None,
                "analise_tokens": 0,
                "analise_erro": "item_invalido_pos_analise",
            }
            continue

        item.setdefault("analise", None)
        item.setdefault("analise_tokens", 0)
        item.setdefault("analise_erro", None)

    tlog(f"[ANALISE] ‚úÖ Conclu√≠do: {total} an√°lises geradas.")
    return resultados

def gerar_embeddings(resultados: List[dict], model: str = "text-embedding-3-small", batch_size: int = 100) -> List[dict]:
    """
    Gera embeddings em BATCHES para cada item combinando caption + transcricao + framesDescricao.
    Retorna embeddings no formato:
      {"embedding": {"vectorized_embedding": [...], "embedding_provider": "openai", "embedding_model": model}}

    batch_size: n√∫mero m√°ximo de textos por chamada √† API (100 recomendado)
    """

    if not resultados:
        return resultados

    _, client, _ = get_clients()

    def limpar_texto(txt: str) -> str:
        """Remove pontua√ß√£o e m√∫ltiplos espa√ßos."""
        txt = re.sub(r"[^A-Za-z√Ä-√ø0-9\s]", " ", str(txt))
        txt = re.sub(r"\s+", " ", txt).strip()
        return txt

    # üîπ Monta a lista de textos a vetorializar
    textos = []
    idx_map = [] # mapeia √≠ndice original ‚Üí posi√ß√£o no batch
    for i, item in enumerate(resultados):
        caption = item.get("caption") or ""
        transcricao = item.get("transcricao") or ""
        frames = item.get("framesDescricao") or ""

        texto = f"caption: {caption} transcricao: {transcricao} frames: {frames}"
        texto = limpar_texto(texto)
        if texto:
            textos.append(texto)
            idx_map.append(i)
        else:
            item["embedding"] = None # sem texto para vetorializar

    if not textos:
        return resultados

    # üîπ Envia em batches (melhor desempenho e custo)
    for start in range(0, len(textos), batch_size):
        end = min(start + batch_size, len(textos))
        batch = textos[start:end]
        idxs = idx_map[start:end]

        try:
            resp = client.embeddings.create(model=model, input=batch)
            embeddings = [d.embedding for d in resp.data]

            for i, emb in zip(idxs, embeddings):
                resultados[i]["embedding"] = {
                    "vectorized_embedding": emb,
                    "embedding_provider": "openai",
                    "embedding_model": model,
                }

        except Exception as e:
            # Em caso de erro no batch, marca todos os itens afetados
            for i in idxs:
                resultados[i]["embedding"] = {
                    "vectorized_embedding": None,
                    "embedding_provider": "openai",
                    "embedding_model": model,
                    "error": str(e),
                }
            tlog(f"[EMBEDDINGS] ‚ö†Ô∏è Erro no batch {start}-{end}: {e}")

    tlog(f"[EMBEDDINGS] ‚úÖ Conclu√≠do: {len(textos)} textos vetorizados em batches de {batch_size}.")
    return resultados

# Nome do √≠ndice vetorial no Atlas
VECTOR_INDEX_NAME = "comm_ref_vector_index"

# Fun√ß√£o para buscar os vizinhos no Mongo
def _buscar_vizinhos_mongo(embedding_vector, k=5):
    """Busca os k vizinhos mais pr√≥ximos no MongoDB Atlas Vector Search."""
    if embedding_vector is None:
        return []

    # Converte para lista se vier como np.ndarray
    if isinstance(embedding_vector, np.ndarray):
        embedding_vector = embedding_vector.tolist()
    elif not isinstance(embedding_vector, list):
        try:
            embedding_vector = list(embedding_vector)
        except Exception:
            print("[CLUSTER] Embedding inv√°lido ‚Äî n√£o iter√°vel.")
            return []

    if len(embedding_vector) == 0:
        return []

    db = _connect_to_mongo()
    collection_ref = db["comunidades-ref"]

    pipeline = [
        {
            "$vectorSearch": {
                "index": VECTOR_INDEX_NAME,
                "path": "embedding", # campo vetorial na cole√ß√£o de refer√™ncia
                "queryVector": embedding_vector,
                "numCandidates": 100,
                "limit": k,
            }
        },
        {
            "$project": {
                "_id": 1,
                "score": {"$meta": "vectorSearchScore"},
                "comunidade": 1,
                "categoria_principal": 1,
                "subcategoria": 1,
            }
        },
    ]

    try:
        resultados = list(collection_ref.aggregate(pipeline))
        return resultados
    except Exception as e:
        print(f"[CLUSTER] Erro ao buscar vizinhos no MongoDB: {e}")
        return []

# Fun√ß√£o para inferir categoria
def _inferir_categoria_knn(vizinhos):
    """Determina comunidade/categoria/subcategoria predominantes ponderando pelos scores."""
    if not vizinhos:
        return None

    comunidades = [v.get("comunidade") for v in vizinhos if v.get("comunidade")]
    categorias = [v.get("categoria_principal") for v in vizinhos if v.get("categoria_principal")]
    subcategorias = [v.get("subcategoria") for v in vizinhos if v.get("subcategoria")]
    scores = np.array([v.get("score", 0) for v in vizinhos], dtype=float)

    def ponderar(valores, distancias, epsilon: float = 1e-9):
        if (
            valores is None
            or len(valores) == 0
            or distancias is None
            or len(distancias) == 0
            or len(valores) != len(distancias)
        ):
            return None, {}
    
        raw = np.array(distancias, dtype=float)
    
        # --------- DETEC√á√ÉO AUTOM√ÅTICA ---------
        # Se os scores est√£o entre 0 e 1 ‚Üí s√£o similaridades
        if np.all((raw >= 0) & (raw <= 1)):
            dist = 1.0 - raw
        else:
            dist = raw
    
        # substitui dist√¢ncias inv√°lidas
        dist[~np.isfinite(dist)] = 1.0
    
        # dist√¢ncia zero ‚Üí peso enorme ‚Üí evitamos
        dist = dist + epsilon
    
        # proximidade = 1/dist√¢ncia
        inv_dist = 1.0 / dist
    
        # pesos normalizados
        pesos_norm = inv_dist / inv_dist.sum()
    
        # soma pesos por categoria
        pesos_dict = {}
        for val, peso in zip(valores, pesos_norm):
            if val:
                pesos_dict[val] = pesos_dict.get(val, 0.0) + float(peso)
    
        if not pesos_dict:
            return None, {}
    
        proporcoes = {k: round(v * 100, 1) for k, v in pesos_dict.items()}
        valor_predito = max(proporcoes, key=proporcoes.get)
    
        return valor_predito, proporcoes

    comunidade_predita, comunidades_prop = ponderar(comunidades, scores)
    categoria_predita, _ = ponderar(categorias, scores)
    subcategoria_predita, _ = ponderar(subcategorias, scores)

    return {
        "comunidade_predita": comunidade_predita,
        "categoria_principal": categoria_predita,
        "subcategoria": subcategoria_predita,
        "comunidades_proporcoes": comunidades_prop,
    }

def _hash_embedding(embedding):
    """Cria uma chave hash √∫nica para logging/debug (n√£o usada em cache)."""
    try:
        return hashlib.md5(np.array(embedding, dtype=float).tobytes()).hexdigest()
    except Exception:
        return "invalid_hash"

def _buscar_vizinhos_mongo_wrapper(embedding, k=5):
    """
    Wrapper simples para validar e converter o embedding antes da busca.
    (Sem cache, totalmente seguro para arrays numpy.)
    """
    if embedding is None:
        return []

    if isinstance(embedding, np.ndarray):
        embedding = embedding.tolist()
    elif not isinstance(embedding, list):
        try:
            embedding = list(embedding)
        except Exception:
            print("[CLUSTER] Embedding inv√°lido recebido no wrapper.")
            return []

    if len(embedding) == 0:
        return []

    try:
        return _buscar_vizinhos_mongo(embedding, k=k)
    except Exception as e:
        print(f"[CLUSTER] Erro em _buscar_vizinhos_mongo_wrapper: {e}")
        return []

def classificar_via_mongo_vector_search(resultados, k=5, max_workers=max_workers):
    """
    Classifica cada item via MongoDB Atlas Vector Search (sem cache),
    garantindo seguran√ßa de tipos e compatibilidade com numpy arrays.
    """

    def processar_item(item):
        emb = None
        try:
            emb = (item.get("embedding") or {}).get("vectorized_embedding")
        except Exception:
            pass

        # Converte para lista se necess√°rio
        if emb is None:
            return item
        if isinstance(emb, np.ndarray):
            emb = emb.tolist()
        elif not isinstance(emb, list):
            try:
                emb = list(emb)
            except Exception:
                print(f"[WARN] embedding inv√°lido no item: {item.get('url')}")
                return item

        if len(emb) == 0:
            return item

        try:
            vizinhos = _buscar_vizinhos_mongo_wrapper(emb, k=k)
            resultado = _inferir_categoria_knn(vizinhos)
            if resultado:
                item.update(resultado)
        except Exception as e:
            print(f"[CLUSTER] Erro ao processar item {item.get('url')}: {e}")

        return item

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futuros = [executor.submit(processar_item, item) for item in resultados]
        resultados_final = [f.result() for f in as_completed(futuros)]

    return resultados_final

def _coletar_caminhos_midia(resultados: List[dict]) -> Set[Path]:
    paths: Set[Path] = set()
    def _add(p):
        if p:
            pp = Path(p)
            # üö® Ajuste: S√≥ adiciona se o arquivo estiver na pasta 'media' ou 'tmp'
            if pp.exists() and pp.is_file() and (str(pp).startswith(media) or str(pp).startswith(str(BASE_DIR / "tmp"))):
                paths.add(pp)
    
    # Adiciona os caminhos de m√≠dia
    for item in resultados:
        _add(item.get("mediaLocalPath"))
        mlist = item.get("mediaLocalPaths")
        if isinstance(mlist, (list, tuple)):
            for p in mlist:
                _add(p)
    
    # Adiciona arquivos tempor√°rios de √°udio (se houver algum que a fun√ß√£o transcrever n√£o removeu a tempo)
    tmpdir = Path(BASE_DIR) / "tmp"
    if tmpdir.exists():
        for f in tmpdir.glob("audio_*"):
            if f.is_file() and (time.time() - f.stat().st_mtime > 5): # Remove arquivos com mais de 5s
                 paths.add(f)
                 
    return paths

def _deletar_arquivos(paths: Iterable[Path]) -> None:
    for p in paths:
        try:
            p.unlink(missing_ok=True)
            tlog(f"[CLEANUP] üóëÔ∏è {p.name} removido por limpeza final.")
        except Exception as e:
            print(f"[CLEANUP] Falha ao deletar {p}: {e}")

def _deletar_pasta_se_vazia(pasta: Path) -> None:
    try:
        if pasta.exists() and pasta.is_dir() and not any(pasta.iterdir()):
            pasta.rmdir()
            tlog(f"[CLEANUP] üóëÔ∏è Pasta vazia {pasta.name} removida.")
    except Exception as e:
        print(f"[CLEANUP] Falha ao remover pasta {pasta}: {e}")

async def fetch_social_post_summary_async(
    post_url: List[str],
    api_token: Optional[str] = None,
    max_results: int = 1000,
) -> List[Dict[str, Optional[str]]]:
    s = get_settings()
    api_token = api_token or s.APIFY_KEY # type: ignore[attr-defined]

    input_urls = [u.strip() for u in post_url if u and u.strip()]
    if not input_urls:
        tlog("[FETCH] Nenhuma URL v√°lida fornecida.")
        return []

    known_map, unknown_set = _partition_known_unknown_by_mongo(input_urls)

    urls_instagram_profiles = [u for u in input_urls if "instagram.com" in u and _is_ig_profile_url(u)]
    urls_instagram_posts = [u for u in input_urls if "instagram.com" in u and _shortcode(u)]
    
    urls_tiktok_profiles = [u for u in input_urls if "tiktok.com" in u and _is_tt_profile_url(u)]
    urls_tiktok_posts = [u for u in input_urls if "tiktok.com" in u and _tt_id(u)]
    
    urls_static = [u for u in input_urls if "https://static-resources" in u]
    
    # filtrar o que j√° est√° no Mongo
    urls_instagram_posts = [u for u in urls_instagram_posts if u in unknown_set]
    urls_tiktok_posts = [u for u in urls_tiktok_posts if u in unknown_set]
    # üö® Ajuste: Se a URL do perfil estiver no Mongo, ela ainda deve ser usada como semente, mas o fan-out filtrar√° posts existentes.
    # urls_tiktok_profiles = [u for u in urls_tiktok_profiles if u in unknown_set] 
    urls_static = [u for u in urls_static if u in unknown_set]

    tlog(
    f"[FETCH] IG posts novos: {len(urls_instagram_posts)} | "
    f"IG perfis: {len(urls_instagram_profiles)} | "
    f"TT posts novos: {len(urls_tiktok_posts)} | "
    f"TT perfis: {len(urls_tiktok_profiles)} | "
    f"Static novos: {len(urls_static)} | J√° no Mongo: {len(known_map)}"
    )

    tasks = []
    if urls_instagram_posts:
        tlog("[FETCH] Agendando fetch IG (posts)‚Ä¶")
        tasks.append(asyncio.to_thread(_fetch_instagram, urls_instagram_posts, api_token, max_results))
    if urls_instagram_profiles:
        tlog("[FETCH] Agendando fetch IG (perfis‚Üíposts)‚Ä¶")
        tasks.append(asyncio.to_thread(_fetch_instagram_from_profiles, urls_instagram_profiles, api_token, 5, max_results))
    if urls_tiktok_posts:
        tlog("[FETCH] Agendando fetch TikTok (posts)‚Ä¶")
        tasks.append(asyncio.to_thread(_fetch_tiktok, urls_tiktok_posts, api_token, max_results))
    if urls_tiktok_profiles:
        tlog("[FETCH] Agendando fetch TikTok (perfis‚Üíposts)‚Ä¶")
        tasks.append(asyncio.to_thread(_fetch_tiktok_from_profiles, urls_tiktok_profiles, api_token, 5, max_results))
    if urls_static:
        tlog("[FETCH] Agendando fetch Static‚Ä¶")
        tasks.append(asyncio.to_thread(_fetch_static_resources, urls_static, max_results))

    fetched_new_items: List[Dict[str, Optional[str]]] = []
    if tasks:
        parts = await asyncio.gather(*tasks, return_exceptions=True)
        for p in parts:
            if isinstance(p, Exception):
                tlog(f"[FETCH] ‚ö†Ô∏è Tarefa paralela falhou: {p}")
                continue
            fetched_new_items.extend(p or [])


    results = _merge_results_in_input_order(input_urls, known_map, fetched_new_items)

    # remove placeholders vazios de perfis
    profile_set = set(_normalize_ig_profile_url(u) for u in urls_instagram_profiles) | \
                  set(_normalize_tt_profile_url(u) for u in urls_tiktok_profiles)

    def _is_placeholder_for_profile(row: dict) -> bool:
        u = row.get("url") or ""
        try:
            if "instagram.com" in u:
                nu = _normalize_ig_profile_url(u)
            elif "tiktok.com" in u:
                nu = _normalize_tt_profile_url(u)
            else:
                nu = u
        except Exception:
            nu = u
        if nu not in profile_set:
            return False
        return all(row.get(k) is None for k in OUTPUT_FIELDS if k != "url")

    results = [r for r in results if not _is_placeholder_for_profile(r)]

    # ‚úÖ garante inclus√£o de posts que n√£o foram mapeados ‚Äî mas IGNORA perfis
    seen_urls = set(r.get("url") for r in results if r.get("url"))
    for it in fetched_new_items:
        u = it.get("url")
        if not u or u in seen_urls:
            continue
        # pula URLs de perfil para n√£o criar "registro do perfil"
        try:
            if ("tiktok.com" in u and _is_tt_profile_url(u)) or ("instagram.com" in u and _is_ig_profile_url(u)):
                continue
        except Exception:
            pass

        row = {k: it.get(k) if k in it else None for k in OUTPUT_FIELDS}
        row["url"] = u
        row["_from_mongo"] = False
        row.setdefault("hashtags", [])
        row.setdefault("mentions", [])
        results.append(row)
        seen_urls.add(u)

    # Deduplica√ß√£o final (url + ownerUsername + timestamp)
    unique: Dict[Tuple[str, Optional[str], Optional[str]], dict] = {}
    for r in results:
        key = (
            str(r.get("url") or "").rstrip("/"),
            r.get("ownerUsername"),
            str(r.get("timestamp") or r.get("post_timestamp") or ""),
        )
        if key not in unique:
            unique[key] = r
    results = list(unique.values())

    if len(results) > max_results:
        results = results[:max_results]

    # Sanitiza√ß√£o final: remove itens None ou n√£o-dicts
    results = [r for r in results if isinstance(r, dict)]

    tlog(f"[FETCH] Finalizado: {len(results)} item(ns) total.")
    
    return results

async def rodar_pipeline(urls: List[str], progress_callback=None) -> List[dict]:
    """
    Executa o pipeline completo:
    1) Busca posts
    2) Transcreve e extrai frames
    3) Gera embeddings
    4) Classifica via Mongo Vector Search
    5) Limpa m√≠dia tempor√°ria
    """
    tlog("[PIPELINE] Iniciando rodar_pipeline()")
    if not urls:
        print("Nenhuma URL fornecida.")
        return []

    total_steps = 5
    step = 0
    final_results: List[dict] = []

    # --- fun√ß√£o auxiliar para reportar progresso normalizado ---
    def _safe_progress(percent: float, msg: str = ""):
        """Garante que o valor do progresso fique entre 0 e 100 e captura erros do Streamlit."""
        if not progress_callback:
            return
        try:
            percent = max(0.0, min(1.0, percent)) # mant√©m entre 0 e 1
            progress_callback(round(percent, 2), msg) # envia 0‚Äì100%
        except Exception as e:
            print(f"[Aviso] progress_callback falhou: {e}")

    def update_step(msg):
        nonlocal step
        step += 1
        _safe_progress(step / total_steps, msg)

    try:
        # ----------------------------
        # 1Ô∏è‚É£ Buscar/baixar posts
        # ----------------------------
        update_step("üîç Buscando e baixando posts...")
        resultados = await fetch_social_post_summary_async(urls, api_token=None, max_results=10000)
        final_results = resultados # mant√©m a refer√™ncia para limpeza no finally
        if not resultados:
            print("Nenhum resultado retornado pelos scrapers.")
            return []

        # ----------------------------
        # 2Ô∏è‚É£ Transcrever e extrair frames
        # ----------------------------
        update_step("üéôÔ∏è Transcrevendo e extraindo frames...")

        # FILTRO CR√çTICO (impede 'NoneType' mais tarde)
        resultados = [r for r in resultados if isinstance(r, dict)]

        total_videos = len([r for r in resultados if r.get('mediaLocalPath')])
        progresso_local = 0

        def local_progress(concluidos=None, total=None):
            nonlocal progresso_local
            progresso_local += 1
            # calcula progresso relativo (0‚Äì100) dentro desta etapa
            progresso_total = (step + (progresso_local / max(1, total_videos))) / total_steps
            _safe_progress(progresso_total, f"üéß Transcrevendo v√≠deos ({progresso_local}/{total_videos})...")

        tlog(f"[PIPELINE] Chamando anexar_transcricoes_threaded() com {len(resultados)} posts")
        tlog("Checando resultados malformados...")
        for i, r in enumerate(resultados):
            if r is None:
                tlog(f"[ERRO] resultados[{i}] == None")
            elif not isinstance(r, dict):
                tlog(f"[ERRO] resultados[{i}] N√ÉO √â dict: {type(r)}")

        
        anexar_transcricoes_threaded(resultados, max_workers=max_workers, gpu_singleton=False, callback=local_progress)
        tlog("[PIPELINE] Finalizou anexar_transcricoes_threaded()")

        # --- FILTRO CR√çTICO AP√ìS A TRANSCRI√á√ÉO ---
        # Em alguns casos, uma thread pode retornar None e sobrescrever um item da lista.
        # Isso evita que gerar_embeddings() ou classificar_via_mongo_vector_search() quebrem.
        posts_iniciais = len(resultados)
        resultados = [r for r in resultados if isinstance(r, dict)]
        filtrados = posts_iniciais - len(resultados)
        if filtrados > 0:
            tlog(f"[WARN] Removidos {filtrados} itens inv√°lidos (None ou n√£o-dict) ap√≥s transcri√ß√£o.")

        # ----------------------------
        # 2Ô∏è‚É£.5Ô∏è Gerar AN√ÅLISE GPT para cada post
        # ----------------------------
        tlog("[PIPELINE] Iniciando anexar_analises_threaded()...")

        def local_progress_analise(concluidos, total):
            # progresso dentro desta etapa, an√°logo ao da transcri√ß√£o
            progresso_total = (step + (concluidos / max(1, total))) / total_steps
            _safe_progress(progresso_total, f"üìù Analisando posts ({concluidos}/{total})...")

        anexar_analises_threaded(
            resultados,
            max_workers=max_workers,
            callback=local_progress_analise,
        )
        tlog("[PIPELINE] Finalizou anexar_analises_threaded()")

        # ----------------------------
        # 3Ô∏è‚É£ Gerar embeddings
        # ----------------------------
        update_step("üß† Gerando embeddings...")
        resultados = gerar_embeddings(resultados)

        # ----------------------------
        # 4Ô∏è‚É£ Classificar via Mongo Vector Search
        # ----------------------------
        update_step("üè∑Ô∏è Classificando resultados...")
        resultados = classificar_via_mongo_vector_search(resultados, k=5)

        # ----------------------------
        # 5Ô∏è‚É£ Limpar arquivos tempor√°rios
        # ----------------------------
        update_step("üßπ Limpando diret√≥rios tempor√°rios...")
        caminhos = _coletar_caminhos_midia(resultados)
        _deletar_arquivos(caminhos)
        _deletar_pasta_se_vazia(Path(media))
        _deletar_pasta_se_vazia(Path(BASE_DIR) / "tmp")

        _safe_progress(1.0, "‚úÖ Finalizado com sucesso!")
        return resultados
    
    except Exception as pipeline_error:
        tlog(f"[ERROR] ‚ùå Erro fatal no pipeline: {pipeline_error}")
        # Garante a limpeza mesmo em caso de erro
        try:
            tlog("[PIPELINE] Tentando limpeza de emerg√™ncia...")
            caminhos = _coletar_caminhos_midia(final_results)
            _deletar_arquivos(caminhos)
            _deletar_pasta_se_vazia(Path(media))
            _deletar_pasta_se_vazia(Path(BASE_DIR) / "tmp")
            tlog("[PIPELINE] Limpeza de emerg√™ncia conclu√≠da.")
        except Exception as cleanup_error:
             tlog(f"[ERROR] Falha na limpeza de emerg√™ncia: {cleanup_error}")
        raise # relan√ßa o erro original



